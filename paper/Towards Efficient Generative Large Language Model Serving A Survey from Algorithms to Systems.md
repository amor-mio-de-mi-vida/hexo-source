---
date: 2024-10-09 13:14:45
date modified: 2024-10-10 23:10:11
title: "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"
tags: 
categories: 
excerpt: LLM çš„ç©ºå‰æˆåŠŸä¹Ÿå¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ï¼Œæœ€æ˜æ˜¾çš„æ˜¯å®ƒä»¬åœ¨æœåŠ¡æœŸé—´çš„å·¨å¤§è®¡ç®—è¦æ±‚ã€‚å·¨å¤§çš„æ¨¡å‹å¤§å°å’Œå¤æ‚æ€§ï¼ŒåŠ ä¸Šå¯¹å¤§é‡è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ã€‚è¿™äº›æ¨¡å‹çš„èµ„æºå¯†é›†å‹æ€§è´¨å¼•å‘äº†å¯¹èƒ½è€—ã€å¯æ‰©å±•æ€§å’Œå¯è®¿é—®æ€§çš„æ‹…å¿§ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨æ²¡æœ‰ä¸°å¯Œè®¡ç®—èµ„æºçš„æ›´å¹¿æ³›ç¤¾åŒºä¸­çš„é‡‡ç”¨ã€‚
---
[Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems (arxiv.org)](https://arxiv.org/pdf/2312.15234)

## Abstract
LLM çš„ç©ºå‰æˆåŠŸä¹Ÿå¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ï¼Œæœ€æ˜æ˜¾çš„æ˜¯å®ƒä»¬åœ¨æœåŠ¡æœŸé—´çš„å·¨å¤§è®¡ç®—è¦æ±‚ã€‚å·¨å¤§çš„æ¨¡å‹å¤§å°å’Œå¤æ‚æ€§ï¼ŒåŠ ä¸Šå¯¹å¤§é‡è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ã€‚è¿™äº›æ¨¡å‹çš„èµ„æºå¯†é›†å‹æ€§è´¨å¼•å‘äº†å¯¹èƒ½è€—ã€å¯æ‰©å±•æ€§å’Œå¯è®¿é—®æ€§çš„æ‹…å¿§ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨æ²¡æœ‰ä¸°å¯Œè®¡ç®—èµ„æºçš„æ›´å¹¿æ³›ç¤¾åŒºä¸­çš„é‡‡ç”¨ã€‚

æœ¬è°ƒæŸ¥çš„ä¸»è¦ç›®çš„æ˜¯å…¨é¢æ¦‚è¿° LLM æœåŠ¡å’Œæ¨ç†çš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬å°†æ ¹æ®ç°æœ‰æŠ€æœ¯çš„åŸºæœ¬æ–¹æ³•å¯¹å…¶è¿›è¡Œç³»ç»Ÿå›é¡¾å’Œåˆ†ç±»ï¼Œçªå‡ºå®ƒä»¬çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚è¯¥è°ƒæŸ¥å°†æ¶µç›–å¹¿æ³›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬è§£ç ç®—æ³•ã€æ¶æ„è®¾è®¡ã€æ¨¡å‹å‹ç¼©ã€ä½ä½é‡åŒ–ã€å¹¶è¡Œè®¡ç®—ã€å†…å­˜ç®¡ç†ã€è¯·æ±‚è°ƒåº¦å’Œå†…æ ¸ä¼˜åŒ–ã€‚
### Structure

## Background

### Transformer-based LLM

ä»æ•°å­¦ä¸Šè®²ï¼ŒTransformer ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æè¿°å¦‚ä¸‹ï¼šå¯¹äºè¾“å…¥åºåˆ—
$$X=[x_1, x_2, ..., x_n]$$
Transformer ä½¿ç”¨ $X$ çš„çº¿æ€§å˜æ¢è®¡ç®—ä¸€ç»„æŸ¥è¯¢ $Q$ã€é”® $K$ å’Œ $valuesV$ã€‚ç„¶åï¼Œè‡ªæˆ‘æ³¨æ„åˆ†æ•°è®¡ç®—ä¸ºï¼š$$\text{Attention}(Q, K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
å…¶ä¸­$d_k$æ˜¯é”®çš„ç»´åº¦ã€‚è¿™ç§æœºåˆ¶å…è®¸æ¨¡å‹ä¸“æ³¨äºè¾“å‡ºæ¯ä¸ªå…ƒç´ çš„ input åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œæ•è·å¤æ‚çš„ä¾èµ–å…³ç³»ï¼Œè€Œä¸ç®¡å®ƒä»¬åœ¨ input sequence ä¸­çš„è·ç¦»å¦‚ä½•ã€‚

Transformer ä¸­çš„å¦ä¸€ä¸ªé‡è¦ç»“æ„æ˜¯å‰é¦ˆç½‘ç»œ ï¼ˆFFNï¼‰ï¼Œå®ƒå­˜åœ¨äº Transformer çš„æ¯ä¸€å±‚ä¸­ï¼Œå¹¶æ˜¾ç€å½±å“å…¶è®¡ç®—å¼ºåº¦ã€‚FFN é€šå¸¸ç”±ä¸¤ä¸ªçº¿æ€§å˜æ¢ç»„æˆï¼Œä¸­é—´æœ‰ä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸è¡¨ç¤ºä¸ºï¼š
$$\text{FFN}(x)=max(0, xW_1+b_1)W_2+b_2$$
å…¶ä¸­ï¼Œ$W_1$ã€$W_2$ã€$b_1$ å’Œ $b_2$ æ˜¯ FFN çš„å¯å­¦ä¹ å‚æ•°ï¼Œéçº¿æ€§å‡½æ•° $maxï¼ˆ0ï¼ŒÂ·ï¼‰$ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º ReLUï¼‰å°†å¿…è¦çš„éçº¿æ€§å¼•å…¥æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„æ¨¡å¼ã€‚FFN è´Ÿè´£æ¨¡å‹å‚æ•°è®¡æ•°çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ï¼Œå› æ­¤è´Ÿè´£å…¶å†…å­˜å ç”¨å’Œè®¡ç®—è´Ÿè½½ã€‚åœ¨æ¯ä¸ª Transformer å±‚ä¸­ï¼Œåœ¨å¤šå¤´æ³¨æ„åŠ› ï¼ˆMHAï¼‰ èšåˆæ¥è‡ªè¾“å…¥ä¸åŒéƒ¨åˆ†çš„ä¿¡æ¯åï¼ŒFFN ä¼šä¸ºæ¯ä¸ªä½ç½®ç‹¬ç«‹å¤„ç†è¿™äº›èšåˆä¿¡æ¯ã€‚è¿™ç§å¹¶è¡Œå¤„ç†èƒ½åŠ›æ˜¯ Transformer çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†åºåˆ—ã€‚ä½†æ˜¯ï¼Œè¿™ä¹Ÿæ„å‘³ç€è®¡ç®—è´Ÿè½½å’Œå†…å­˜éœ€æ±‚éšè¾“å…¥åºåˆ—çš„é•¿åº¦å’Œç½‘ç»œçš„æ·±åº¦è€Œå˜åŒ–ã€‚

åœ¨åŸºäº Transformer çš„ LLM ä¸­ï¼Œè‡ªæˆ‘æ³¨æ„å’Œ FFN çš„ç»“åˆä½¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿæ•è·å¹¿æ³›çš„è¯­è¨€ä¸Šä¸‹æ–‡å’Œç»†å¾®å·®åˆ«ï¼Œä»è€Œåœ¨å„ç§ NLP ä»»åŠ¡ä¸­è®¾å®šæ–°çš„åŸºå‡†ã€‚ç„¶è€Œï¼Œè®­ç»ƒå’Œæ¨ç†çš„å¤§é‡è®¡ç®—è¦æ±‚å·²æˆä¸ºä¸€ä¸ªå…³é”®çš„ç ”ç©¶é¢†åŸŸï¼Œä¸“æ³¨äºåœ¨ä¸æ˜¾è‘—å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹ä¼˜åŒ–è¿™äº›æ–¹é¢ã€‚Transformer æ¨¡å‹è¿˜åŒ…æ‹¬å…¶ä»–å…³é”®ç»„ä»¶ï¼Œå¦‚ä½ç½®ç¼–ç ï¼Œå®ƒæ·»åŠ äº†æœ‰å…³åºåˆ—ä¸­æ¯ä¸ªæ ‡è®°ä½ç½®çš„ä¿¡æ¯ï¼Œä»¥åŠå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒå…è®¸æ¨¡å‹å…³æ³¨ä¸åŒè¡¨ç¤ºç©ºé—´ä¸­åºåˆ—çš„ä¸åŒéƒ¨åˆ†ã€‚

### GPUs and Other Accelerators

### LLM Inference

å…ˆå‰çš„ç ”ç©¶å¯¹åŸºäºTransformerçš„LLMæ¨ç†çš„ç®—æ³•å¼ºåº¦è¿›è¡Œäº†æ·±å…¥åˆ†æï¼ˆä¾‹å¦‚ï¼Œè®¡ç®—æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€I/Oå’Œå†…å­˜æ¶ˆè€—ï¼‰ï¼Œå¹¶æ ¹æ®è‡ªå›å½’è§£ç ç®—æ³•çš„æ‰§è¡Œæä¾›äº†å¹¿æ³›çš„å®è¯ç»“æœè¿›è¡Œæˆæœ¬ä¼°ç®—ï¼ˆä¾‹å¦‚ï¼Œå»ºæ¨¡æ¨ç†å»¶è¿Ÿï¼‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„ä¼˜åŒ–æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œå› ä¸ºå¯èƒ½å­˜åœ¨ä¸åŒçš„æœ€ä¼˜ç­–ç•¥ï¼Œä¸åŒçš„ç®—æ³•é…ç½®å’Œç³»ç»Ÿè®¾ç½®ã€‚

### Challenges

- **å»¶è¿Ÿå’Œå“åº”æ—¶é—´**
	é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†éœ€è¦å®ç°ä½å»¶è¿Ÿå’Œå¿«é€Ÿå“åº”æ—¶é—´ï¼Œå°¤å…¶æ˜¯åœ¨èŠå¤©æœºå™¨äººã€è™šæ‹ŸåŠ©æ‰‹å’Œäº¤äº’å¼ç³»ç»Ÿç­‰å®æ—¶åº”ç”¨ç¨‹åºä¸­ã€‚å¹³è¡¡æ¨¡å‹å¤æ‚æ€§å’Œæ¨ç†é€Ÿåº¦æ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œéœ€è¦ä¼˜åŒ–ç®—æ³•å’Œç³»ç»Ÿæ¶æ„ï¼Œä»¥ä¾¿åœ¨ä¸å½±å“å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æœ€å¤§é™åº¦åœ°å‡å°‘å“åº”æ—¶é—´ã€‚
- **å†…å­˜å ç”¨å’Œæ¨¡å‹å¤§å°**
	å¤§å‹è¯­è¨€æ¨¡å‹ç”±äºå…¶åºå¤§çš„ä½“ç§¯å’ŒåŒ…å«çš„å¤§é‡å‚æ•°ï¼Œå¯¹å†…å­˜æœ‰ç€æ˜¾è‘—çš„éœ€æ±‚ã€‚åœ¨å†…å­˜å—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²è¿™ç±»æ¨¡å‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™è¦æ±‚å¼€å‘æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©æŠ€æœ¯å’Œç³»ç»Ÿä¼˜åŒ–æªæ–½ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚
- **å¯æ‰©å±•æ€§å’Œååé‡**
	æ¨ç†ç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç»å¸¸é¢ä¸´ä¸åŒçº§åˆ«çš„è¯·æ±‚è´Ÿè½½ã€‚ç¡®ä¿å¯æ‰©å±•æ€§å’Œé«˜ååé‡ä»¥æœ‰æ•ˆåœ°å¤„ç†å¤šä¸ªåŒæ—¶è¯·æ±‚éœ€è¦å¹¶è¡Œè®¡ç®—ã€è¯·æ±‚è°ƒåº¦å’Œå…¶ä»–ç³»ç»Ÿçº§ä¼˜åŒ–ï¼Œä»¥ä¾¿åœ¨èµ„æºä¹‹é—´æœ‰æ•ˆåœ°åˆ†é…è®¡ç®—å·¥ä½œè´Ÿè½½ã€‚
- **ç¡¬ä»¶å…¼å®¹æ€§å’ŒåŠ é€Ÿ**
	æœ‰æ•ˆåœ°åˆ©ç”¨ç¡¬ä»¶èµ„æºå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è‡³å…³é‡è¦ã€‚å°†å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”äºå¤šæ ·åŒ–çš„ç¡¬ä»¶å¹³å°å’Œæ¶æ„ï¼ŒåŒ…æ‹¬ä¸­å¤®å¤„ç†å™¨ï¼ˆCPUsï¼‰ã€å›¾å½¢å¤„ç†å™¨ï¼ˆGPUsï¼‰å’Œä¸“ä¸šåŠ é€Ÿå™¨ï¼Œéœ€è¦ç¡¬ä»¶æ„ŸçŸ¥çš„ç®—æ³•è®¾è®¡å’Œä¼˜åŒ–ï¼Œä»¥å……åˆ†åˆ©ç”¨åº•å±‚ç¡¬ä»¶çš„æ½œåŠ›ã€‚
-   **å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡**
	ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„æ•ˆç‡æœ‰æ—¶å¯èƒ½æ¶‰åŠåˆ°ä¸æ¨¡å‹å‡†ç¡®æ€§çš„æƒè¡¡ã€‚åœ¨æ¨¡å‹å¤§å°ã€è®¡ç®—å¤æ‚æ€§å’Œæ€§èƒ½ä¹‹é—´æ‰¾åˆ°æ­£ç¡®çš„å¹³è¡¡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™éœ€è¦ä»”ç»†è€ƒè™‘å’Œè¯„ä¼°å„ç§ç®—æ³•å’Œç³»ç»Ÿçº§æŠ€æœ¯ã€‚
	
## Taxonomy
![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/Pasted-image-20241009150811.58hcqf64j1.webp)

### Algorithmic Innovation
![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/Pasted-image-20241009151413.4jo36eilim.webp)

#### Decoding Algorithm
- **éè‡ªå›å½’è§£ç **
	`idea`: æ”¾å¼ƒè‡ªå›å½’ç”ŸæˆèŒƒå¼ï¼Œå¹¶è¡Œè§£ç è¾“å‡ºtokenã€‚åœ¨è§£ç è¿‡ç¨‹ä¸­æ‰“ç ´å•è¯ä¾èµ–æ€§ï¼Œå¹¶å‡è®¾ä¸€å®šç¨‹åº¦çš„æ¡ä»¶ç‹¬ç«‹æ€§ã€‚
	Parallel Decoding of Conditional Masked Language Models.
	Non-autoregressive neural machine translation
	Non-autoregressive neural machine translation with enhanced decoder input.
	
	`idea`: é€šè¿‡å»ºæ¨¡è¾“å‡ºä¾èµ–æ€§æˆ–è¿­ä»£ç»†åŒ–è¾“å‡ºä»¤ç‰Œï¼Œä»¥è¾¾åˆ°è‡ªå›å½’æ¨¡å‹çš„è´¨é‡ã€‚
	Semi-autoregressive training improves mask-predict decoding.
	Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade
	Improving Non-autoregressive Translation with Dependency-Aware Decoder.
	Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement.
	
	`idea`: å—çŠ¶å¹¶è¡Œè§£ç åœ¨åŸºç¡€LLMä¸­æ’å…¥ä¸€ä¸ªå•ä¸€çš„å‰é¦ˆå±‚ï¼Œä»¥å¹¶è¡Œé¢„æµ‹å¤šä¸ªæœªæ¥ä½ç½®ï¼Œç„¶åå›é€€åˆ°ç”±åŸºç¡€æ¨¡å‹éªŒè¯çš„æœ€é•¿å‰ç¼€ã€‚æœ€è¿‘çš„ä¸€äº›åŠªåŠ›è‡´åŠ›äºåœ¨ä¸€æ­¥è§£ç ä¸­ç”Ÿæˆå¤šä¸ªä»¤ç‰Œï¼Œè€Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•è®­ç»ƒæˆ–ä¿®æ”¹ã€‚
	Blockwise parallel decoding for deep autoregressive models.
	Accelerating Transformer Inference for Translation via Parallel Decoding.
		
	`survey`: A survey on non autoregressive generation for neural machine translation and beyond.

- æ¨æµ‹æ€§è§£ç 
	`idea`: æ¨æµ‹æ€§æ‰§è¡Œæ¥åº”å¯¹é¡ºåºæ‰§è¡Œçš„é™åˆ¶ï¼Œå¹¶æé«˜è§£ç çš„å¹¶è¡Œæ€§ã€‚åœ¨è‡ªå›å½’LLMæ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸ªè§£ç æ­¥éª¤éƒ½å¯ä»¥è¢«è§†ä¸ºæ‰§è¡Œä¸€ä¸ªå¸¦æœ‰æ¡ä»¶åˆ†æ”¯çš„ç¨‹åºã€‚è¿™ä¸ªæ–¹æ³•æ¥è‡ªäºè¿™æ ·ä¸€ä¸ªäº‹å®ï¼šé¢„æµ‹çš„è¾“å‡ºæ€»æ˜¯ç”±åŸå§‹LLMéªŒè¯ï¼Œè€Œå½“é¢„æµ‹å‡ºé”™æ—¶ï¼Œå›é€€æœºåˆ¶ä¼šç”Ÿæ•ˆã€‚
	Speculative computation, parallelism, and functional programming.
	Accelerating large language model decoding with speculative sampling.
	Fast inference from transformers via speculative decoding.
	optimization idea: é€šè¿‡å¼•å…¥å¤šä¸ªå°å‹è‰ç¨¿æ¨¡å‹ï¼Œå¹¶ç»“åˆä¸€ç§æ–°é¢–çš„åŸºäºæ ‘çš„æ¨æµ‹æ€§æ¨ç†å’ŒtokenéªŒè¯æœºåˆ¶ã€‚
	SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.
	å›é€€æœºåˆ¶ï¼šBig little transformer decoder

- æ—©æœŸé€€å‡ºæœºåˆ¶
	`idea`: åŸºäºæ—©æœŸæ¨¡å‹å±‚çš„è¾“å‡ºæœ‰å¯èƒ½è‡ªä¿¡åœ°æ¨æ–­å‡ºç›®æ ‡åˆ†å¸ƒã€‚å®ƒä»¬å¯ä»¥åŸºäºå†…éƒ¨åˆ†ç±»å™¨å‘å‡ºé¢„æµ‹ï¼Œè€Œä¸æ˜¯è¿è¡Œæ•´ä¸ªLLMã€‚
	Fast inference via early exiting from deep neural networks.
	Magic pyramid: Accelerating inference with early exiting and token pruning.
	Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting
	A global past-future early exit method for accelerating inference of pre-trained language models.
	FastBERT: a Self-distilling BERT with Adaptive Inference Time
	A simple hash-based early exiting approach for language understanding and generation
	DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference
	TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference
	Learning to Skip for Language Modeling.
	Bert loses patience: Fast and robust inference with early exit.
	SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference.
	Consistent Accelerated Inference via Confident Adaptive Transformers.

- çº§è”æ¨ç†
	`idea`: çº§è”æ¨ç†é‡‡ç”¨ä¸€ç³»åˆ—è§„æ¨¡ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æœ€å°åŒ–å“åº”æ—¶é—´ã€‚å°†å®ƒä»¬ä»¥çº§è”æ–¹å¼ç»„ç»‡èµ·æ¥ï¼Œå¹¶æ ¹æ®å®ä¾‹éš¾åº¦è‡ªé€‚åº”åœ°é€‰æ‹©åˆé€‚çš„åˆ†ç±»å™¨ã€‚
	Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade.
	Tabi: An Efficient Multi-Level Inference System for Large Language Models.
	FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance.
	On Optimal Caching and Model Multiplexing for Large Model Inference.
	LARGE LANGUAGE MODEL CASCADES WITH MIXTURE OF THOUGHT REPRESENTATIONS FOR COST-EFFICIENT REASONING
	Chain-of-thought prompting elicits reasoning in large language models.
	Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.
#### Architecture Design
work: Simplifying Transformer Blocks.
- é…ç½®ç¼©å‡ï¼š
	`idea`: ç¼©å‡æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨æµ…å±‚ç¼–ç å™¨æˆ–è§£ç å™¨
	PoWER-BERT: Accelerating BERT inference via progressive word-vector elimination.
	Adapler: Speeding up inference by adaptive length reduction.
	
	`idea`: æƒé‡å…±äº«ï¼Œè¯æ±‡è¡¨ç¼©å‡
	Shallow Decoder: Reevaluating Non-autoregressive Machine Translation.
- æ³¨æ„åŠ›ç®€åŒ–ï¼š
	è‡ªæ³¨æ„åŠ›è®¡ç®—çš„ä¸€ä¸ªçªå‡ºæŒ‘æˆ˜æ˜¯è®¡ç®—å¤æ‚åº¦$O(ğ¿^2)$ï¼Œå®ƒä¸è¾“å…¥åºåˆ—é•¿åº¦$ğ¿$æˆäºŒæ¬¡æ–¹å…³ç³»ã€‚ä¸ºäº†åº”å¯¹éå¸¸é•¿çš„åºåˆ—ä»»åŠ¡ï¼Œéœ€è¦å°†è¿™äº›æ ‡å‡†æ³¨æ„åŠ›ç®€åŒ–ä¸ºæ›´é«˜æ•ˆçš„é€‰æ‹©ã€‚
	`survey` : Efficient Transformers: A Survey.
	Big bird: Transformers for longer sequences.
	Transformers are rnns: Fast autoregressive transformers with linear attention
	Linformer: Self-attention with linear complexity
	
	`idea`: å€Ÿé‰´å…ˆå‰çš„æ³¨æ„åŠ›ç®€åŒ–æ–¹æ³•ï¼Œå°†å®ƒä»¬æ¦‚æ‹¬å’Œç»“åˆï¼Œä»¥ç¼©çŸ­ä¸Šä¸‹æ–‡å¹¶å‡å°‘KVç¼“å­˜çš„å¤§å°ï¼Œä»¥åŠé™ä½æ³¨æ„åŠ›å¤æ‚åº¦
	Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer.
	Mistral 7B.
	Faster Causal Attention Over Large Sequences Through Sparse Flash Attention
	Longnet: Scaling transformers to 1,000,000,000 tokens.
	
	`idea`: é€šè¿‡å°†ä¸Šä¸‹æ–‡å‹ç¼©æˆæ›´å°‘çš„è½¯tokenæ¥è¿›è¡Œä¸Šä¸‹æ–‡å‹ç¼©
	Adapting Language Models to Compress Contexts.
	Landmark Attention: Random-Access Infinite Context Length for Transformers.
	In-context autoencoder for context compression in a large language model.
	CacheGen: Fast Context Loading for Language Model Applications.
	
	`idea`: æ ¹æ®ä¸åŒçš„é‡è¦æ€§æŒ‡å¯¼ç›´æ¥ä¸¢å¼ƒæˆ–é‡æ–°è¡¨è¿°ä¸é‡è¦çš„ä¸Šä¸‹æ–‡token
	Extending Context Window of Large Language Models via Semantic Compression.
	Llmlingua: Compressing prompts for accelerated inference of large language models.
	Compressing Context to Enhance Inference Efficiency of Large Language Models.
	Learning to compress prompts with gist tokens.
	Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers.
	Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time.
	H\_2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.
	Efficient Streaming Language Models with Attention Sinks.
	Longformer: The long-document transformer.

è¡¨1å±•ç¤ºäº†å››ç§ä»£è¡¨æ€§æ–¹æ³•çš„ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼åŠå…¶åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºä¸Šä¸‹æ–‡ä¸å®Œæ•´ï¼Œè¿™äº›æ–¹æ³•åœ¨å®é™…å·¥ä½œè´Ÿè½½ä¸­å¯èƒ½é¢ä¸´ä¸å¯é¿å…çš„ä¿¡æ¯æŸå¤±ï¼Œç‰¹åˆ«æ˜¯å½“æ³¨æ„åŠ›åˆ†å¸ƒæ›´å¤æ‚æ—¶ã€‚
![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/image.5q7eh98kv4.webp)
- æ¿€æ´»å…±äº«ï¼šé€šè¿‡å…±äº«ä¸­é—´æ¿€æ´»æ¥æé«˜æ³¨æ„åŠ›è®¡ç®—çš„æ•ˆç‡ã€‚
	`idea`: æ³¨æ„åŠ›å…±äº«æ–¹æ³•è§‚å¯Ÿåˆ°ä¸åŒå±‚çš„æ³¨æ„åŠ›çŸ©é˜µåˆ†å¸ƒä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶é‡æ–°ä½¿ç”¨è¿™äº›æ³¨æ„åŠ›çŸ©é˜µä»¥å‡å°‘è®¡ç®—æˆæœ¬ã€‚
	An efficient transformer decoder with compressed sub-layers.
	Speeding up Transformer Decoding via an Attention Refinement Network.
	Sharing Attention Weights for Fast Transformer.
	
	`idea`: å¤šæŸ¥è¯¢æ³¨æ„åŠ›(MQA)ä½¿å¾—ä¸åŒçš„å¤´å…±äº«åŒä¸€ç»„é”®å’Œå€¼ï¼Œä»¥å‡å°‘å¢é‡æ¨ç†ä¸­çš„å†…å­˜å¸¦å®½éœ€æ±‚ã€‚
	Fast transformer decoding: One write-head is all you need.
	
	`idea`: ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)æ”¾å®½äº†å•ä¸€ç»„é”®å’Œå€¼çš„é™åˆ¶åˆ°å¤šç»„ï¼Œå¹¶ä¸”æ¯ç»„ä¸ä¸€ç»„æŸ¥è¯¢è€¦åˆã€‚
	GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.
- æ¡ä»¶è®¡ç®—ï¼š
	ç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰èŒƒå¼å°†æ¨¡å‹çš„èƒ½åŠ›åˆ†å¸ƒåœ¨å„ç§â€œä¸“å®¶â€ä¸Šï¼Œè¿™äº›â€œä¸“å®¶â€æ˜¯æ›´å°çš„ç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªä¸“å®¶ä¸“æ³¨äºæ•°æ®çš„ä¸åŒå­é›†ã€‚å®ƒå…è®¸ç³»ç»Ÿä»…æ ¹æ®æŸäº›è·¯ç”±æœºåˆ¶è°ƒç”¨ç»™å®šè¾“å…¥æ‰€éœ€çš„ä¸“å®¶ï¼Œè€Œä¸æ˜¯åœ¨æ•´ä¸ªå¤§å‹æ¨¡å‹ä¸Šè®¡ç®—ï¼Œä»è€Œå®ç°äº†è®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚
	SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention.
	Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
	Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
	GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.
	Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate.
	Hash layers for large sparse models.
	Memory Augmented Language Models through Mixture of Word Experts.
	Mixture-of-experts with expert choice routing.
	Glam: Efficient scaling of language models with mixture-of-experts.
	
	Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference.
	MoEçš„åŠ¨æ€ç‰¹æ€§ä¹Ÿè¦æ±‚ç‰¹æ®Šçš„ç³»ç»Ÿä¼˜åŒ–ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼é€šä¿¡GPUå†…æ ¸å®ç°ï¼Œä»¥ä¿ƒè¿›MoEæ¨ç†æ•ˆç‡ã€‚
	FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models.
	Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference.
	Tutel: Adaptive mixture-of-experts at scale.
	Accelerating Distributed {MoE} Training and Inference with Lina.
	FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement.
	Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale.
	MegaBlocks: Efficient Sparse Training with Mixture-of-Experts.
	PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation.
- å¾ªç¯å•å…ƒ
	å°½ç®¡é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰åœ¨æ•æ‰åºåˆ—ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½†ä»æœ‰å‡ ç§æ–¹æ³•ä½¿ç”¨é€’å½’å•å…ƒæ¥æ›¿æ¢Transformeræ¨¡å—ï¼Œå¹¶åœ¨æ¨ç†æœŸé—´å®ç°çº¿æ€§çš„è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦ã€‚
	RWKV: Reinventing RNNs for the Transformer Era.
	Retentive Network: A Successor to Transformer for Large Language Models.
	`idea`: è¿™äº›æœ€è¿‘çš„æ¢ç´¢å¤§å¤šå»ºç«‹åœ¨çº¿æ€§æ³¨æ„åŠ›è¡¨ç¤ºä¹‹ä¸Šã€‚ç»è¿‡é‡ç»„åï¼Œå®ƒä»¬é€šè¿‡ä½¿ç”¨çº¿æ€§é€’å½’å•å…ƒå¯¹tokenä¹‹é—´çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå…‹æœäº†æ³¨æ„åŠ›çš„$O(L^2)$ç“¶é¢ˆï¼Œè¿™äº›é€’å½’å•å…ƒæ›´å®¹æ˜“ä¿æŒå¯å¹¶è¡ŒåŒ–è®­ç»ƒçš„æ€§è´¨ã€‚
	Transformers are rnns: Fast autoregressive transformers with linear attention.
	An attention free transformer.
	Hungry Hungry Hippos: Towards Language Modeling with State Space Models.
	Mamba: Linear-Time Sequence Modeling with Selective State Spaces.
	Efficiently Modeling Long Sequences with Structured State Spaces.
	Long Range Language Modeling via Gated State Spaces.
	Resurrecting recurrent neural networks for long sequences.
	`idea`: è®¾è®¡è¿˜åŒ…æ‹¬å„ç§ä½ç½®ç¼–ç æ¨¡å—ï¼ŒæŒ‡æ•°è¡°å‡æœºåˆ¶ä»¥åŠä¸€ç³»åˆ—tokençº§åˆ«çš„éçº¿æ€§MLPsæˆ–GLUsï¼Œä»¥æ”¹è¿›æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
	Roformer: Enhanced transformer with rotary position embedding.
	The statistical recurrent unit.
	Mlp-mixer: An all-mlp architecture for vision.
	Metaformer is actually what you need for vision.
	Language modeling with gated convolutional networks.
#### Model Compression
- çŸ¥è¯†è’¸é¦ï¼š
	`idea`: é€šè¿‡å¤§å‹æ•™å¸ˆæ¨¡å‹çš„ç›‘ç£æ¥è®­ç»ƒä¸€ä¸ªå°å‹å­¦ç”Ÿæ¨¡å‹ã€‚å¤§å¤šæ•°å…ˆå‰æ–¹æ³•éƒ½åœ¨æ¢ç´¢ç™½ç›’è’¸é¦ï¼Œè¿™éœ€è¦è®¿é—®æ•´ä¸ªæ•™å¸ˆæ¨¡å‹çš„å‚æ•°ã€‚
	Knowledge Distillation of Large Language Models.
	TinyBERT: Distilling BERT for Natural Language Understanding.
	DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.
	Patient Knowledge Distillation for BERT Model Compression
	Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.
	`idea`: é»‘ç›’è’¸é¦
	Stanford alpaca: An instruction-following llama model.
	Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.
	Wizardlm: Empowering large language models to follow complex instructions.
	Instruction tuning with gpt-4.
	Minigpt-4: Enhancing vision-language understanding with advanced large language models.
	OpenAI. 2023. GPT-4 Technical Report.
- ç½‘ç»œå‰ªæ: 
	ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œè¿™ç§»é™¤äº†æ•´ä¸ªç»“æ„åŒ–çš„LLMç»„ä»¶ï¼Œä¾¿äºGPUåŠ é€Ÿã€‚
	Reducing Transformer Depth on Demand with Structured Dropout.
	Ziplm: Hardware-aware structured pruning of language models.
	LLM-Pruner: On the Structural Pruning of Large Language Models.
	What Matters In The Structured Pruning of Generative Language Models?
	Deja vu: Contextual sparsity for efficient llms at inference time.
	éç»“æ„åŒ–æ–¹æ³•ï¼šå®ƒä»¬é€šå¸¸å®ç°50-60%çš„ç¨€ç–åº¦ä»¥å‹ç¼©LLMsã€‚å¯ä»¥è¿›ä¸€æ­¥æ³›åŒ–åˆ°åŠç»“æ„åŒ–çš„N:Mç¨€ç–ï¼ˆå³2:4å’Œ4:8ï¼‰ï¼Œåˆ©ç”¨NVIDIAç¨€ç–å¼ é‡æ ¸å¿ƒçš„åŠ é€Ÿï¼Œæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ã€‚
	Accelerating sparse deep neural networks.
	LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation.
	DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization.
	Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity.
	PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU.

### System Optimization


## Software Frameworks


## Benchmarks



## Connection with other surveys





## Future Direction



