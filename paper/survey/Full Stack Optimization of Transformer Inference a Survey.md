---
title: "Full Stack Optimization of Transformer Inference: a Survey"
date: 2024-10-13 21:22:09
tags:
  - Hexo
  - Keep
categories:
  - Hexo
date modified: 2024-10-17 17:03:00
---
最近Transformer模型在进行推理时所需的计算量和带宽正在以显著的速度增长，这使得它们在延迟敏感型应用中的部署变得具有挑战性。因此，人们越来越关注如何提高Transformer模型的效率，方法从改变架构设计，到开发专门的领域特定加速器不等。在这项工作中我们调研了不同的高效Transformer推理方法，包括：

(i) 分析和剖析现有Transformer架构中的瓶颈，以及它们与之前的卷积模型在相似性和差异性；

(ii) Transformer架构对硬件的影响，包括非线性运算如层归一化（Layer Normalization）、Softmax和GELU，以及线性运算对硬件设计的影响；

(iii) 优化固定Transformer架构的方法；

(iv) 在为Transformer模型寻找正确的操作映射和调度方面所面临的挑战；以及(v) 通过使用神经架构搜索来适应架构，优化Transformer模型的方法。

最后，我们通过对Gemmini（一个开源的全栈深度神经网络加速器生成器）应用所调研的优化方法进行了一个案例研究，并展示了每种方法相比于之前在Gemmini上的基准测试结果所带来的改进。

<!-- more-->

## Introduction

对快速、高效计算的需求，使用少量不同操作的特点，以及数据重用的机会，都导致了深度学习硬件加速器的使用。

与传统的以CNN为重点的设计相比，transformer 主要由矩阵乘法（matmuls）和内存密集型非线性操作组成。此外，transformer 模型的计算图和数据流比CNNs更加复杂，操作节点类型更多，数据流的分裂和连接也更为复杂。所有这些挑战都要求我们全面分析当前的硬件和软件解决方案，以及 transformer 推理的各种设计权衡。进行这样的分析将使我们能够建立对高效运行 transformer 的全面和深入的理解。

本文包含了一项调查和分析，涵盖了端到端深度学习推理中的不同层次，特别是关注Transformers 。这包括以下内容：

• 分析和评估 transformer 架构的运行时特性和瓶颈。 深入研究 transformer 在执行过程中的性能表现，识别出影响推理速度的关键因素。

• 用于 transformer 推理的硬件架构，包括 transformer 架构中的非线性操作对设计的影响。探讨不同的硬件设计如何适应 transformer 的计算需求，特别是如何处理 transformer 中的复杂操作。

• 通过剪枝和量化等优化策略进一步提高固定 transformer 架构的性能。论如何通过减少模型大小和降低精度来提升推理效率，同时保持模型的准确性。

• 在 transformer 架构中操作映射和调度的方法以及相关的挑战。分析如何有效地将 transformer 的操作分配到硬件资源上，以及在这个过程中遇到的难题。

• 通过自动化神经架构搜索过程设计并调整 transformer 架构，使其更加硬件高效。探讨如何利用算法来自动寻找和优化 transformer 架构，以便在给定的硬件上实现最佳性能。

通过这些内容，本文旨在提供一个全面的视角，帮助理解 transformer 推理的各个方面，并为未来的研究和开发提供指导。

我们的案例研究将调查到的方法应用于部署 transformer ，得出了一些关键发现，包括以下内容：

• Gemmini，最初是为CNN工作负载设计的，并不适合 transformer 推理的硬件加速器架构。在CNN领域专用加速器上运行 transformer 的主要瓶颈不一定是线性操作，而是花费在浮点非线性操作以及量化和反量化操作上的时间。除非这些操作得到适当的处理，否则可能会导致硬件利用率低于1%。

• 对于 transformer 加速器来说，通常拥有更大的累加器大小和更小的暂存器大小是更好的，而对于CNN加速器来说，相反的配置往往更优。根据这一观察结果改变加速器架构，可以比针对CNN基准优化的基线实现36%的延迟改进（第3.4.3节）。

• 尽管 transformer 中的矩阵乘法调度只需要3个循环，而CNN中的卷积需要6个，但我们发现为 transformer 找到高效的调度与为CNN找到高效的调度一样具有挑战性。为 transformer 选择合适的调度决策涉及大量决策，最佳和最差解决方案的性能差异可达四个数量级。

• 在CNN模型中将批量归一化与相邻的卷积融合是直接的。然而，在 transformer 架构中将层归一化与前置的矩阵乘法融合时，会对映射施加约束，特别是与 tile 大小相关的约束。这需要进一步考虑，因为在某些情况下，由于映射约束导致的运行时成本可能会超过操作融合带来的收益。

### Transformer

![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/image.1e8lgwgxuy.webp)

### Nonlinear Operations.

非线性操作，如Softmax、LayerNorm和GELU，需要专门的支持或片外计算。与线性操作相比，在推断变换器网络时，这些非线性操作在整体操作中占比较小（见第2.2.2节）。然而，它们在典型硬件上的计算比矩阵乘法（matmuls）更具挑战性，如果处理不当，可能会产生显著的开销。

非线性操作在有效利用临时内存和高效计算方面存在挑战。这是因为它们需要对所有输入值进行多次遍历，这要求这些值必须保存在临时内存中。如图2（a）所示，Softmax操作涉及（1）指数运算，（2）跨序列长度维度求和结果，（3）通过除以求和结果来规范化输入。众所周知，指数函数容易产生数值溢出，因此需要使用最大值减法技巧，将表达式 exp(𝑥𝑖)/Í 𝑗 exp(𝑥𝑗) 转换为 exp(𝑥𝑖 −𝑥max)/Í 𝑗 exp(𝑥𝑗 −𝑥max)，其中 𝑥max 是 𝑥𝑗 的最大值。然而，这需要对输入进行额外的遍历，导致一个三遍历的数值稳定实现。计算LayerNorm函数也需要对整个隐藏维度上的输入值进行多次遍历，如图2（b）所示。在第一遍历中，必须计算均值。在第二遍历中，使用这个均值来计算标准差。最后，在第三遍历中，实际应用规范化时，每个输入值都需要进行一次除法。

![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/image.231v0x7azh.webp)

此外，非线性操作在操作融合方面也带来了挑战，操作融合是一种常见技术，通过将多个操作合并为单个操作来减少层间通信（见第5.2.1节）。与许多CNN架构中的批量归一化（BatchNorm）可以无缝地合并到前驱或后继的线性操作中不同，LayerNorm需要在实际运行时计算输入的均值和方差。因此，要将这个操作与前面的矩阵乘法操作融合，必须在整个输出矩阵上跨缩减维度（即计算均值和方差的维度）累加结果，然后写出结果。这导致了不规则的 tile 大小和较低的数据重用。因此，将这些操作与前一层的融合与使用更好的 tile 大小以最大化重用之间存在非平凡的权衡。

### 编码器-解码器架构

在编码器块中，推理由矩阵-矩阵乘法以及元素级的加法和非线性操作组成。MHA模块和FFN模块中的投影层的成本与输入序列长度𝑙线性相关。然而，MHA模块中的act-to-act矩阵乘法与序列长度的平方成正比。我们通过分析表明，这取决于序列长度：在短序列长度时，投影层占主导，使得编码器块的整体复杂度为$𝑂(𝑙)$；而在长序列长度时，act-to-act矩阵乘法占主导，使得整体复杂度为$𝑂(𝑙^2)$。

![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/image.pfbx37gra.webp)

解码器块。与仅编码器模型相比，由重复解码器块组成的仅解码器模型本质上是自回归的。投影操作仅应用于输入令牌，结果是一个矩阵-向量乘法，并且成本是恒定的。然而，这并不适用于act-to-act矩阵乘法，因为输入令牌并不独立于先前生成的令牌。相反，它需要关注所有先前生成的令牌。因此，这些操作与序列长度成线性关系，这意味着处理更大时间步中的令牌比处理更小时间步中的令牌需要更多的计算。一个关键的细节是，为了使输入令牌能够关注所有先前生成的令牌，必须存在完整的键（key）和值（value）激活。

令牌生成的一个常见优化技术是缓存并重用先前生成令牌的中间键和值，在后续迭代中避免重新计算它们。总的来说，生成整个序列的端到端复杂性对于投影层是线性的，而对于其他两个act-to-act矩阵乘法是平方的。

### Summary
  
变换器由几个变换器块组成，每个变换器块都有一个MHA（多头注意力模块）和一个FFN（前馈网络）模块（每个模块后都有LayerNorm和残差加法）。MHA模块包含投影层以及激活到激活的矩阵乘法和Softmax操作。FFN模块由两个投影层组成，它们之间有一个非线性函数。变换器块有两种类型：编码器块和解码器块。编码器块可以并行处理整个输入序列，使其适用于自然语言理解任务。解码器块是自回归的，意味着每次生成输出标记时都必须进行一次推理，因此通常用于生成性任务。

## 模型分析

### 工作负载分析

为了评估Transformer中的瓶颈，我们首先建模了计算仅编码器Transformer模型和仅解码器Transformer模型所需的浮点运算次数（FLOPs），以及这些网络的算术强度。算术强度是指每从内存中加载一个字节可以执行的浮点运算次数。它可以通过将总FLOPs数除以总访问字节数（也称为MOPs，或内存操作）来计算：

$$
\text{Arighmetic Intensiry} = \frac{\text{\#FLOPs}}{\text{\#MOPs}}
$$
  
在这里，我们假设本地内存足够大，可以完全在内存中保存给定操作的两个矩阵，因此计算出的算术强度值作为可达到数据重用的上限。在计算FLOPs时，我们还分别计算乘法和加法操作，即使它们来自一个MAC（乘法和累加）操作。

端到端的FLOPs和MOPs。在编码器分析中，我们使用了12层的BERT-Base模型和24层的BERT-Large网络（见表1中的模型配置）；对于解码器，我们使用了12层的GPT-2模型架构，该架构具有与BERT-Base相同的模型配置参数。为了分析的目的，在本节中我们忽略了标准BERT模型的最大输入序列长度512。然后，我们计算了在推断这些模型时必须访问的字节数，即MOPs。我们假设所有操作都是8位精度，这意味着加载一个参数或激活将需要加载一个字节。对于解码器模型，我们测量了FLOPs和MOPs，即迭代生成给定长度的完整序列所需的浮点操作和内存操作的总数。这些网络在不同序列长度下的FLOPs和MOPs分别绘制在图4和图5中。可以看出，对于所有模型，FLOPs和MOPs都呈超线性增长，尤其是在长序列长度范围内，这是由于在act-to-act矩阵乘法中与序列长度的二次复杂度所致。

端到端计算强度。然后，我们通过将推断这些模型所需的FLOPs数量除以MOPs的数量来模拟算术强度。BERT-Base、BERT-Large和GPT-2相对于序列长度的算术强度如图6所示。对于BERT-Base和BERT-Large，算术强度最初随着序列长度的增加而增加，直到512，然后在更长的序列长度之后减少。之所以会出现这种情况，是因为正如在第2.2.2节中更详细的分析，对于短序列，具有比MHA模块更高算术强度的FFN模块（见表3）在总FLOPs中占主导地位（见图7）。然而，对于更长的序列长度，这种趋势会逆转，因为MHA模块中的act-to-act矩阵乘法成本会随着序列长度的增加而二次方增长，导致端到端模型推断的算术强度降低。

与仅编码器的BERT推断相比，仅解码器的GPT-2推断表现出显著较低的算术强度。这是由于解码器仅由矩阵-向量运算组成，这限制了数据重用的机会。也就是说，对于单个矩阵-向量运算，我们大约为加载的每个参数执行一次乘法和加法操作。加载的负载无法在令牌之间共享。这导致大约为加载的每个参数执行两次操作。重要的是要注意，随着序列长度的增加，GPT-2的FLOPs比BERT-Base和BERT-Large要少。然而，由于其算术强度较低，通常更难以高效地运行其推断。这使得其性能受限于内存带宽，与仅编码器的BERT模型相比。

逐层FLOPs、MOPs和算术强度。然后，我们评估了BERT-Base编码器（表3）的逐层FLOPs、MOPs和算术强度与序列长度的关系。如表3所示，随着序列长度的增加，act-to-act矩阵乘法消耗的FLOPs和MOPs比例增加，这些操作的算术强度低于FFN和MHA模块中的投影层。这解释了仅编码器模型在长序列长度下的整体算术强度下降，如图6所示。

act-to-act矩阵乘法相对于投影层的低算术强度是因为这两个操作中的𝑑/ℎ维度相对于投影层的维度（𝑑和𝑑𝐹𝐹𝑁）以及相对于𝑙（序列长度）来说较小。小的矩阵维度导致较低的算术强度，因为每个矩阵元素要执行的操作较少，导致重用减少。大激活尺寸的加载和存储进一步加剧了低算术强度，这个激活尺寸不仅随着序列长度𝑙的平方增长，而且还乘以头数ℎ，因为在多头方案中每个头都有自己的激活（注意力分数）。因此，如附录A.3中的表10所示，一个假设的BERT模型如果头数较少（从而𝑑/ℎ维度较大），将减少MOPs的数量并提高MHA模块中act-to-act注意力的算术强度。这表明在设计Transformer架构时，头数可以在准确性和硬件性能指标之间构成一种权衡。

此外，表3说明虽然非线性操作（表中归类为“其他”）消耗的总体FLOPs较少，但它们消耗了大量的MOPs，尤其是在较长的序列长度上。与act-to-act矩阵乘法的情况类似，长序列长度下Softmax操作中的大量MOPs主要是由于每个注意力头必须写入或加载的多个𝑙×𝑙矩阵。这也表明，如果处理不当，非线性激活可能会成为整体性能的显著贡献者，尽管它们对总FLOPs的贡献可能微不足道。我们在附录A.3的表11中对GPT-2解码器进行了类似的逐层分析，该分析表明与仅编码器模型相比，所有层的算术强度显著降低，这是由于大量的内存操作造成的。

与ResNet50的比较。为了提供一个典型CNN的FLOPs、MOPs和算术强度的基线，我们还包括了ResNet50的相应分析（架构细节可以在附录A.2中找到）。表4提供了ResNet50的FLOPs、MOPs和算术强度的细分。与序列长度为128的BERT-Base编码器（表3）相比，没有操作融合的ResNet50消耗的FLOPs减少了3.07倍，MOPs减少了1.28倍，导致其端到端的算术强度低于表3中所有序列长度的BERT-Base。低算术强度部分是由于ResNet50中的非线性操作消耗的FLOPs比例微不足道，但消耗了显著的MOPs比例，这与BERT-Base编码器类似。然而，与Transformers中的非线性操作不同，ResNet50中的这些操作可以以简单的方式与之前的矩阵乘法融合以进行推理。特别是，ReLU操作可以直接应用于累积的输出，而BatchNorm操作实际上可以折叠到之前的卷积中。融合ReLU消除了这个操作的MOPs，而折叠BatchNorm消除了这个操作所需的FLOPs和MOPs。广义上讲，操作融合是指一种方法，其中一个操作（例如，矩阵乘法或卷积）的输出值直接用作后续操作（例如，ReLU或BatchNorm）的输入，而无需首先将输出值写入片外内存。操作融合消除了非线性操作不必要的内存加载和存储需求，因此进一步提高了端到端的算术强度。如表4所示，将这些操作与之前的卷积融合，将ResNet-50网络的总体算术强度从66.9提高到121.4。在附录A.4的表12中，我们提供了ResNet50几个卷积层的FLOPs、MOPs和算术强度的更详细数据作为参考。

请注意，算术强度大致估计了不同模型和操作在理想情况下可能实现的数据重用程度。稍后在第3.3节中，我们将讨论分析建模可以通过考虑硬件细节来提供更准确、非理想的估计。

2.2.2 分析。为了分析商品硬件上Transformer工作负载的瓶颈，我们在Intel Gold 6242 CPU上对Transformer推理进行了分析。我们分析了仅编码器BERT-Base和仅解码器GPT-2的工作负载延迟细分。

延迟细分。图7和图8分别展示了在CPU上BERT-Base和GPT-2的延迟细分如何随序列长度变化。这些细分表明，对于短序列长度（例如，128-512），大部分计算在FFN模块的投影层中，而MHA计算的大部分也在投影层中。然而，随着序列长度的增加，act-to-act矩阵乘法开始占据主导地位，因为它们都与序列长度的平方成比例增长。

端到端延迟。图9显示了BERT-Base、BERT-Large和GPT-2不同序列长度的归一化延迟。很明显，对于每个序列长度，GPT-2的延迟远远长于BERT-Base或BERT-Large的延迟，尽管BERT-Base和GPT-2在模型配置和端到端FLOPs方面大体相同（如图4所示）。这主要是由于矩阵向量操作的较低算术强度，这在图6中得到了突出显示。具有较高算术强度的模型可以在相同的（甚至更多的）FLOPs下比具有较低算术强度的模型运行得更快。这些观察结果证实了我们的发现，即解码器推理是一个内存受限的问题，而不是计算受限的问题。我们将在第4.3.3节重新讨论这个问题，以讨论一些现有方法来加速解码过程。