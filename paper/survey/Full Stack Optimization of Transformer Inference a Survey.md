---
title: "Full Stack Optimization of Transformer Inference: a Survey"
date: 2024-10-13 21:22:09
tags:
  - Hexo
  - Keep
categories:
  - Hexo
date modified: 2024-10-17 13:52:10
---
最近Transformer模型在进行推理时所需的计算量和带宽正在以显著的速度增长，这使得它们在延迟敏感型应用中的部署变得具有挑战性。因此，人们越来越关注如何提高Transformer模型的效率，方法从改变架构设计，到开发专门的领域特定加速器不等。在这项工作中我们调研了不同的高效Transformer推理方法，包括：

(i) 分析和剖析现有Transformer架构中的瓶颈，以及它们与之前的卷积模型在相似性和差异性；

(ii) Transformer架构对硬件的影响，包括非线性运算如层归一化（Layer Normalization）、Softmax和GELU，以及线性运算对硬件设计的影响；

(iii) 优化固定Transformer架构的方法；

(iv) 在为Transformer模型寻找正确的操作映射和调度方面所面临的挑战；以及(v) 通过使用神经架构搜索来适应架构，优化Transformer模型的方法。

最后，我们通过对Gemmini（一个开源的全栈深度神经网络加速器生成器）应用所调研的优化方法进行了一个案例研究，并展示了每种方法相比于之前在Gemmini上的基准测试结果所带来的改进。

<!-- more-->

## Introduction

对快速、高效计算的需求，使用少量不同操作的特点，以及数据重用的机会，都导致了深度学习硬件加速器的使用。

与传统的以CNN为重点的设计相比，transformer 主要由矩阵乘法（matmuls）和内存密集型非线性操作组成。此外，transformer 模型的计算图和数据流比CNNs更加复杂，操作节点类型更多，数据流的分裂和连接也更为复杂。所有这些挑战都要求我们全面分析当前的硬件和软件解决方案，以及 transformer 推理的各种设计权衡。进行这样的分析将使我们能够建立对高效运行 transformer 的全面和深入的理解。

本文包含了一项调查和分析，涵盖了端到端深度学习推理中的不同层次，特别是关注Transformers 。这包括以下内容：

• 分析和评估 transformer 架构的运行时特性和瓶颈。 深入研究 transformer 在执行过程中的性能表现，识别出影响推理速度的关键因素。

• 用于 transformer 推理的硬件架构，包括 transformer 架构中的非线性操作对设计的影响。探讨不同的硬件设计如何适应 transformer 的计算需求，特别是如何处理 transformer 中的复杂操作。

• 通过剪枝和量化等优化策略进一步提高固定 transformer 架构的性能。论如何通过减少模型大小和降低精度来提升推理效率，同时保持模型的准确性。

• 在 transformer 架构中操作映射和调度的方法以及相关的挑战。分析如何有效地将 transformer 的操作分配到硬件资源上，以及在这个过程中遇到的难题。

• 通过自动化神经架构搜索过程设计并调整 transformer 架构，使其更加硬件高效。探讨如何利用算法来自动寻找和优化 transformer 架构，以便在给定的硬件上实现最佳性能。

通过这些内容，本文旨在提供一个全面的视角，帮助理解 transformer 推理的各个方面，并为未来的研究和开发提供指导。

我们的案例研究将调查到的方法应用于部署 transformer ，得出了一些关键发现，包括以下内容：

• Gemmini，最初是为CNN工作负载设计的，并不适合 transformer 推理的硬件加速器架构。在CNN领域专用加速器上运行 transformer 的主要瓶颈不一定是线性操作，而是花费在浮点非线性操作以及量化和反量化操作上的时间。除非这些操作得到适当的处理，否则可能会导致硬件利用率低于1%。

• 对于 transformer 加速器来说，通常拥有更大的累加器大小和更小的暂存器大小是更好的，而对于CNN加速器来说，相反的配置往往更优。根据这一观察结果改变加速器架构，可以比针对CNN基准优化的基线实现36%的延迟改进（第3.4.3节）。

• 尽管 transformer 中的矩阵乘法调度只需要3个循环，而CNN中的卷积需要6个，但我们发现为 transformer 找到高效的调度与为CNN找到高效的调度一样具有挑战性。为 transformer 选择合适的调度决策涉及大量决策，最佳和最差解决方案的性能差异可达四个数量级。

• 在CNN模型中将批量归一化与相邻的卷积融合是直接的。然而，在 transformer 架构中将层归一化与前置的矩阵乘法融合时，会对映射施加约束，特别是与 tile 大小相关的约束。这需要进一步考虑，因为在某些情况下，由于映射约束导致的运行时成本可能会超过操作融合带来的收益。

### Transformer

![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/image.1e8lgwgxuy.webp)

### Nonlinear Operations.

非线性操作，如Softmax、LayerNorm和GELU，需要专门的支持或片外计算。与线性操作相比，在推断变换器网络时，这些非线性操作在整体操作中占比较小（见第2.2.2节）。然而，它们在典型硬件上的计算比矩阵乘法（matmuls）更具挑战性，如果处理不当，可能会产生显著的开销。

非线性操作在有效利用临时内存和高效计算方面存在挑战。这是因为它们需要对所有输入值进行多次遍历，这要求这些值必须保存在临时内存中。如图2（a）所示，Softmax操作涉及（1）指数运算，（2）跨序列长度维度求和结果，（3）通过除以求和结果来规范化输入。众所周知，指数函数容易产生数值溢出，因此需要使用最大值减法技巧，将表达式 exp(𝑥𝑖)/Í 𝑗 exp(𝑥𝑗) 转换为 exp(𝑥𝑖 −𝑥max)/Í 𝑗 exp(𝑥𝑗 −𝑥max)，其中 𝑥max 是 𝑥𝑗 的最大值。然而，这需要对输入进行额外的遍历，导致一个三遍历的数值稳定实现。计算LayerNorm函数也需要对整个隐藏维度上的输入值进行多次遍历，如图2（b）所示。在第一遍历中，必须计算均值。在第二遍历中，使用这个均值来计算标准差。最后，在第三遍历中，实际应用规范化时，每个输入值都需要进行一次除法。

![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/image.231v0x7azh.webp)

此外，非线性操作在操作融合方面也带来了挑战，操作融合是一种常见技术，通过将多个操作合并为单个操作来减少层间通信（见第5.2.1节）。与许多CNN架构中的批量归一化（BatchNorm）可以无缝地合并到前驱或后继的线性操作中不同，LayerNorm需要在实际运行时计算输入的均值和方差。因此，要将这个操作与前面的矩阵乘法操作融合，必须在整个输出矩阵上跨缩减维度（即计算均值和方差的维度）累加结果，然后写出结果。这导致了不规则的 tile 大小和较低的数据重用。因此，将这些操作与前一层的融合与使用更好的 tile 大小以最大化重用之间存在非平凡的权衡。

### 编码器-解码器架构

在编码器块中，推理由矩阵-矩阵乘法以及元素级的加法和非线性操作组成。MHA模块和FFN模块中的投影层的成本与输入序列长度𝑙线性相关。然而，MHA模块中的act-to-act矩阵乘法与序列长度的平方成正比。我们通过分析表明，这取决于序列长度：在短序列长度时，投影层占主导，使得编码器块的整体复杂度为$𝑂(𝑙)$；而在长序列长度时，act-to-act矩阵乘法占主导，使得整体复杂度为$𝑂(𝑙^2)$。

解码器块。与仅编码器模型相比，由重复解码器块组成的仅解码器模型本质上是自回归的。投影操作仅应用于输入令牌，结果是一个矩阵-向量乘法，并且成本是恒定的。然而，这并不适用于act-to-act矩阵乘法，因为输入令牌并不独立于先前生成的令牌。相反，它需要关注所有先前生成的令牌。因此，这些操作与序列长度成线性关系，这意味着处理更大时间步中的令牌比处理更小时间步中的令牌需要更多的计算。一个关键的细节是，为了使输入令牌能够关注所有先前生成的令牌，必须存在完整的键（key）和值（value）激活。

令牌生成的一个常见优化技术是缓存并重用先前生成令牌的中间键和值，在后续迭代中避免重新计算它们。总的来说，生成整个序列的端到端复杂性对于投影层是线性的，而对于其他两个act-to-act矩阵乘法是平方的。

### Summary
  
变换器由几个变换器块组成，每个变换器块都有一个MHA（多头注意力模块）和一个FFN（前馈网络）模块（每个模块后都有LayerNorm和残差加法）。MHA模块包含投影层以及激活到激活的矩阵乘法和Softmax操作。FFN模块由两个投影层组成，它们之间有一个非线性函数。变换器块有两种类型：编码器块和解码器块。编码器块可以并行处理整个输入序列，使其适用于自然语言理解任务。解码器块是自回归的，意味着每次生成输出标记时都必须进行一次推理，因此通常用于生成性任务。

## 模型分析

### 工作负载分析

为了评估Transformer中的瓶颈，我们首先建模了计算仅编码器Transformer模型和仅解码器Transformer模型所需的浮点运算次数（FLOPs），以及这些网络的算术强度。算术强度是指每从内存中加载一个字节可以执行的浮点运算次数。它可以通过将总FLOPs数除以总访问字节数（也称为MOPs，或内存操作）来计算：

$$
\text{Arighmetic Intensiry} = \frac{\text{\#FLOPs}}{\text{\#MOPs}}
$$
  
在这里，我们假设本地内存足够大，可以完全在内存中保存给定操作的两个矩阵，因此计算出的算术强度值作为可达到数据重用的上限。在计算FLOPs时，我们还分别计算乘法和加法操作，即使它们来自一个MAC（乘法和累加）操作。

端到端的FLOPs和MOPs。在编码器分析中，我们使用了12层的BERT-Base模型和24层的BERT-Large网络（见表1中的模型配置）；对于解码器，我们使用了12层的GPT-2模型架构，该架构具有与BERT-Base相同的模型配置参数。为了分析的目的，在本节中我们忽略了标准BERT模型的最大输入序列长度512。然后，我们计算了在推断这些模型时必须访问的字节数，即MOPs。我们假设所有操作都是8位精度，这意味着加载一个参数或激活将需要加载一个字节。对于解码器模型，我们测量了FLOPs和MOPs，即迭代生成给定长度的完整序列所需的浮点操作和内存操作的总数。这些网络在不同序列长度下的FLOPs和MOPs分别绘制在图4和图5中。可以看出，对于所有模型，FLOPs和MOPs都呈超线性增长，尤其是在长序列长度范围内，这是由于在act-to-act矩阵乘法中与序列长度的二次复杂度所致。

