---
date: 2024-10-09 13:14:45
date modified: 2024-10-13 21:18:30
title: "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems"
tags:
  - paper
  - survey
categories: " paper"
---
LLM çš„ç©ºå‰æˆåŠŸå¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ï¼Œæœ€æ˜æ˜¾çš„æ˜¯å®ƒä»¬åœ¨æœåŠ¡æœŸé—´çš„å·¨å¤§è®¡ç®—è¦æ±‚ã€‚å·¨å¤§çš„æ¨¡å‹å¤§å°å’Œå¤æ‚æ€§ï¼ŒåŠ ä¸Šå¯¹å¤§é‡è®¡ç®—èµ„æºçš„éœ€æ±‚ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„å¹¿æ³›éƒ¨ç½²ã€‚è¿™äº›æ¨¡å‹çš„èµ„æºå¯†é›†å‹æ€§è´¨å¼•å‘äº†å¯¹èƒ½è€—ã€å¯æ‰©å±•æ€§å’Œå¯è®¿é—®æ€§çš„æ‹…å¿§ï¼Œé˜»ç¢äº†å®ƒä»¬åœ¨æ²¡æœ‰ä¸°å¯Œè®¡ç®—èµ„æºçš„æ›´å¹¿æ³›ç¤¾åŒºä¸­çš„é‡‡ç”¨ã€‚

æœ¬è°ƒæŸ¥çš„ä¸»è¦ç›®çš„æ˜¯å…¨é¢æ¦‚è¿° LLM æœåŠ¡å’Œæ¨ç†çš„æœ€æ–°è¿›å±•ã€‚æˆ‘ä»¬å°†æ ¹æ®ç°æœ‰æŠ€æœ¯çš„åŸºæœ¬æ–¹æ³•å¯¹å…¶è¿›è¡Œç³»ç»Ÿå›é¡¾å’Œåˆ†ç±»ï¼Œçªå‡ºå®ƒä»¬çš„ä¼˜åŠ¿å’Œå±€é™æ€§ã€‚è¯¥è°ƒæŸ¥å°†æ¶µç›–å¹¿æ³›çš„æ–¹æ³•ï¼ŒåŒ…æ‹¬è§£ç ç®—æ³•ã€æ¶æ„è®¾è®¡ã€æ¨¡å‹å‹ç¼©ã€ä½ä½é‡åŒ–ã€å¹¶è¡Œè®¡ç®—ã€å†…å­˜ç®¡ç†ã€è¯·æ±‚è°ƒåº¦å’Œå†…æ ¸ä¼˜åŒ–ã€‚

[Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems (arxiv.org)](https://arxiv.org/pdf/2312.15234)

<!-- more -->

### Structure

## Background

### Transformer-based LLM

ä»æ•°å­¦ä¸Šè®²ï¼ŒTransformer ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æè¿°å¦‚ä¸‹ï¼šå¯¹äºè¾“å…¥åºåˆ—
$$X=[x_1, x_2, ..., x_n]$$
Transformer ä½¿ç”¨ $X$ çš„çº¿æ€§å˜æ¢è®¡ç®—ä¸€ç»„æŸ¥è¯¢ $Q$ã€é”® $K$ å’Œ $valuesV$ã€‚ç„¶åï¼Œè‡ªæˆ‘æ³¨æ„åˆ†æ•°è®¡ç®—ä¸ºï¼š$$\text{Attention}(Q, K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$
å…¶ä¸­$d_k$æ˜¯é”®çš„ç»´åº¦ã€‚è¿™ç§æœºåˆ¶å…è®¸æ¨¡å‹ä¸“æ³¨äºè¾“å‡ºæ¯ä¸ªå…ƒç´ çš„ input åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œæ•è·å¤æ‚çš„ä¾èµ–å…³ç³»ï¼Œè€Œä¸ç®¡å®ƒä»¬åœ¨ input sequence ä¸­çš„è·ç¦»å¦‚ä½•ã€‚

Transformer ä¸­çš„å¦ä¸€ä¸ªé‡è¦ç»“æ„æ˜¯å‰é¦ˆç½‘ç»œ ï¼ˆFFNï¼‰ï¼Œå®ƒå­˜åœ¨äº Transformer çš„æ¯ä¸€å±‚ä¸­ï¼Œå¹¶æ˜¾ç€å½±å“å…¶è®¡ç®—å¼ºåº¦ã€‚FFN é€šå¸¸ç”±ä¸¤ä¸ªçº¿æ€§å˜æ¢ç»„æˆï¼Œä¸­é—´æœ‰ä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œé€šå¸¸è¡¨ç¤ºä¸ºï¼š
$$\text{FFN}(x)=max(0, xW_1+b_1)W_2+b_2$$
å…¶ä¸­ï¼Œ$W_1$ã€$W_2$ã€$b_1$ å’Œ $b_2$ æ˜¯ FFN çš„å¯å­¦ä¹ å‚æ•°ï¼Œéçº¿æ€§å‡½æ•° $maxï¼ˆ0ï¼ŒÂ·ï¼‰$ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º ReLUï¼‰å°†å¿…è¦çš„éçº¿æ€§å¼•å…¥æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ æ›´å¤æ‚çš„æ¨¡å¼ã€‚FFN è´Ÿè´£æ¨¡å‹å‚æ•°è®¡æ•°çš„å¾ˆå¤§ä¸€éƒ¨åˆ†ï¼Œå› æ­¤è´Ÿè´£å…¶å†…å­˜å ç”¨å’Œè®¡ç®—è´Ÿè½½ã€‚åœ¨æ¯ä¸ª Transformer å±‚ä¸­ï¼Œåœ¨å¤šå¤´æ³¨æ„åŠ› ï¼ˆMHAï¼‰ èšåˆæ¥è‡ªè¾“å…¥ä¸åŒéƒ¨åˆ†çš„ä¿¡æ¯åï¼ŒFFN ä¼šä¸ºæ¯ä¸ªä½ç½®ç‹¬ç«‹å¤„ç†è¿™äº›èšåˆä¿¡æ¯ã€‚è¿™ç§å¹¶è¡Œå¤„ç†èƒ½åŠ›æ˜¯ Transformer çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿ï¼Œä½¿å…¶èƒ½å¤Ÿæœ‰æ•ˆåœ°å¤„ç†åºåˆ—ã€‚ä½†æ˜¯ï¼Œè¿™ä¹Ÿæ„å‘³ç€è®¡ç®—è´Ÿè½½å’Œå†…å­˜éœ€æ±‚éšè¾“å…¥åºåˆ—çš„é•¿åº¦å’Œç½‘ç»œçš„æ·±åº¦è€Œå˜åŒ–ã€‚

åœ¨åŸºäº Transformer çš„ LLM ä¸­ï¼Œè‡ªæˆ‘æ³¨æ„å’Œ FFN çš„ç»“åˆä½¿è¿™äº›æ¨¡å‹èƒ½å¤Ÿæ•è·å¹¿æ³›çš„è¯­è¨€ä¸Šä¸‹æ–‡å’Œç»†å¾®å·®åˆ«ï¼Œä»è€Œåœ¨å„ç§ NLP ä»»åŠ¡ä¸­è®¾å®šæ–°çš„åŸºå‡†ã€‚ç„¶è€Œï¼Œè®­ç»ƒå’Œæ¨ç†çš„å¤§é‡è®¡ç®—è¦æ±‚å·²æˆä¸ºä¸€ä¸ªå…³é”®çš„ç ”ç©¶é¢†åŸŸï¼Œä¸“æ³¨äºåœ¨ä¸æ˜¾è‘—å½±å“æ€§èƒ½çš„æƒ…å†µä¸‹ä¼˜åŒ–è¿™äº›æ–¹é¢ã€‚Transformer æ¨¡å‹è¿˜åŒ…æ‹¬å…¶ä»–å…³é”®ç»„ä»¶ï¼Œå¦‚ä½ç½®ç¼–ç ï¼Œå®ƒæ·»åŠ äº†æœ‰å…³åºåˆ—ä¸­æ¯ä¸ªæ ‡è®°ä½ç½®çš„ä¿¡æ¯ï¼Œä»¥åŠå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒå…è®¸æ¨¡å‹å…³æ³¨ä¸åŒè¡¨ç¤ºç©ºé—´ä¸­åºåˆ—çš„ä¸åŒéƒ¨åˆ†ã€‚

### GPUs and Other Accelerators

### LLM Inference

å…ˆå‰çš„ç ”ç©¶å¯¹åŸºäºTransformerçš„LLMæ¨ç†çš„ç®—æ³•å¼ºåº¦è¿›è¡Œäº†æ·±å…¥åˆ†æï¼ˆä¾‹å¦‚ï¼Œè®¡ç®—æµ®ç‚¹è¿ç®—æ¬¡æ•°ã€I/Oå’Œå†…å­˜æ¶ˆè€—ï¼‰ï¼Œå¹¶æ ¹æ®è‡ªå›å½’è§£ç ç®—æ³•çš„æ‰§è¡Œæä¾›äº†å¹¿æ³›çš„å®è¯ç»“æœè¿›è¡Œæˆæœ¬ä¼°ç®—ï¼ˆä¾‹å¦‚ï¼Œå»ºæ¨¡æ¨ç†å»¶è¿Ÿï¼‰ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†çš„ä¼˜åŒ–æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œå› ä¸ºå¯èƒ½å­˜åœ¨ä¸åŒçš„æœ€ä¼˜ç­–ç•¥ï¼Œä¸åŒçš„ç®—æ³•é…ç½®å’Œç³»ç»Ÿè®¾ç½®ã€‚

### Challenges

- **å»¶è¿Ÿå’Œå“åº”æ—¶é—´**
	é«˜æ•ˆçš„å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†éœ€è¦å®ç°ä½å»¶è¿Ÿå’Œå¿«é€Ÿå“åº”æ—¶é—´ï¼Œå°¤å…¶æ˜¯åœ¨èŠå¤©æœºå™¨äººã€è™šæ‹ŸåŠ©æ‰‹å’Œäº¤äº’å¼ç³»ç»Ÿç­‰å®æ—¶åº”ç”¨ç¨‹åºä¸­ã€‚å¹³è¡¡æ¨¡å‹å¤æ‚æ€§å’Œæ¨ç†é€Ÿåº¦æ˜¯ä¸€é¡¹å…³é”®æŒ‘æˆ˜ï¼Œéœ€è¦ä¼˜åŒ–ç®—æ³•å’Œç³»ç»Ÿæ¶æ„ï¼Œä»¥ä¾¿åœ¨ä¸å½±å“å‡†ç¡®æ€§çš„æƒ…å†µä¸‹æœ€å¤§é™åº¦åœ°å‡å°‘å“åº”æ—¶é—´ã€‚
- **å†…å­˜å ç”¨å’Œæ¨¡å‹å¤§å°**
	å¤§å‹è¯­è¨€æ¨¡å‹ç”±äºå…¶åºå¤§çš„ä½“ç§¯å’ŒåŒ…å«çš„å¤§é‡å‚æ•°ï¼Œå¯¹å†…å­˜æœ‰ç€æ˜¾è‘—çš„éœ€æ±‚ã€‚åœ¨å†…å­˜å—é™çš„è®¾å¤‡ä¸Šéƒ¨ç½²è¿™ç±»æ¨¡å‹æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œè¿™è¦æ±‚å¼€å‘æœ‰æ•ˆçš„æ¨¡å‹å‹ç¼©æŠ€æœ¯å’Œç³»ç»Ÿä¼˜åŒ–æªæ–½ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¸ç‰ºç‰²æ€§èƒ½ã€‚
- **å¯æ‰©å±•æ€§å’Œååé‡**
	æ¨ç†ç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç»å¸¸é¢ä¸´ä¸åŒçº§åˆ«çš„è¯·æ±‚è´Ÿè½½ã€‚ç¡®ä¿å¯æ‰©å±•æ€§å’Œé«˜ååé‡ä»¥æœ‰æ•ˆåœ°å¤„ç†å¤šä¸ªåŒæ—¶è¯·æ±‚éœ€è¦å¹¶è¡Œè®¡ç®—ã€è¯·æ±‚è°ƒåº¦å’Œå…¶ä»–ç³»ç»Ÿçº§ä¼˜åŒ–ï¼Œä»¥ä¾¿åœ¨èµ„æºä¹‹é—´æœ‰æ•ˆåœ°åˆ†é…è®¡ç®—å·¥ä½œè´Ÿè½½ã€‚
- **ç¡¬ä»¶å…¼å®¹æ€§å’ŒåŠ é€Ÿ**
	æœ‰æ•ˆåœ°åˆ©ç”¨ç¡¬ä»¶èµ„æºå¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†è‡³å…³é‡è¦ã€‚å°†å¤§å‹è¯­è¨€æ¨¡å‹é€‚åº”äºå¤šæ ·åŒ–çš„ç¡¬ä»¶å¹³å°å’Œæ¶æ„ï¼ŒåŒ…æ‹¬ä¸­å¤®å¤„ç†å™¨ï¼ˆCPUsï¼‰ã€å›¾å½¢å¤„ç†å™¨ï¼ˆGPUsï¼‰å’Œä¸“ä¸šåŠ é€Ÿå™¨ï¼Œéœ€è¦ç¡¬ä»¶æ„ŸçŸ¥çš„ç®—æ³•è®¾è®¡å’Œä¼˜åŒ–ï¼Œä»¥å……åˆ†åˆ©ç”¨åº•å±‚ç¡¬ä»¶çš„æ½œåŠ›ã€‚
-   **å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡**
	ä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„æ•ˆç‡æœ‰æ—¶å¯èƒ½æ¶‰åŠåˆ°ä¸æ¨¡å‹å‡†ç¡®æ€§çš„æƒè¡¡ã€‚åœ¨æ¨¡å‹å¤§å°ã€è®¡ç®—å¤æ‚æ€§å’Œæ€§èƒ½ä¹‹é—´æ‰¾åˆ°æ­£ç¡®çš„å¹³è¡¡æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œè¿™éœ€è¦ä»”ç»†è€ƒè™‘å’Œè¯„ä¼°å„ç§ç®—æ³•å’Œç³»ç»Ÿçº§æŠ€æœ¯ã€‚

## Taxonomy
![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/Pasted-image-20241009150811.58hcqf64j1.webp)

### Algorithmic Innovation
![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/Pasted-image-20241009151413.4jo36eilim.webp)


#### Decoding Algorithm
- **éè‡ªå›å½’è§£ç **
- 
	`idea`: æ”¾å¼ƒè‡ªå›å½’ç”ŸæˆèŒƒå¼ï¼Œå¹¶è¡Œè§£ç è¾“å‡ºtokenã€‚åœ¨è§£ç è¿‡ç¨‹ä¸­æ‰“ç ´å•è¯ä¾èµ–æ€§ï¼Œå¹¶å‡è®¾ä¸€å®šç¨‹åº¦çš„æ¡ä»¶ç‹¬ç«‹æ€§ã€‚
	
	Parallel Decoding of Conditional Masked Language Models.
	
	Non-autoregressive neural machine translation
	
	Non-autoregressive neural machine translation with enhanced decoder input.
	
	`idea`: é€šè¿‡å»ºæ¨¡è¾“å‡ºä¾èµ–æ€§æˆ–è¿­ä»£ç»†åŒ–è¾“å‡ºä»¤ç‰Œï¼Œä»¥è¾¾åˆ°è‡ªå›å½’æ¨¡å‹çš„è´¨é‡ã€‚
	
	Semi-autoregressive training improves mask-predict decoding.
	
	Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade
	
	Improving Non-autoregressive Translation with Dependency-Aware Decoder.
	
	Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement.
	
	`idea`: å—çŠ¶å¹¶è¡Œè§£ç åœ¨åŸºç¡€LLMä¸­æ’å…¥ä¸€ä¸ªå•ä¸€çš„å‰é¦ˆå±‚ï¼Œä»¥å¹¶è¡Œé¢„æµ‹å¤šä¸ªæœªæ¥ä½ç½®ï¼Œç„¶åå›é€€åˆ°ç”±åŸºç¡€æ¨¡å‹éªŒè¯çš„æœ€é•¿å‰ç¼€ã€‚æœ€è¿‘çš„ä¸€äº›åŠªåŠ›è‡´åŠ›äºåœ¨ä¸€æ­¥è§£ç ä¸­ç”Ÿæˆå¤šä¸ªä»¤ç‰Œï¼Œè€Œæ— éœ€å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•è®­ç»ƒæˆ–ä¿®æ”¹ã€‚
	
	Blockwise parallel decoding for deep autoregressive models.
	
	Accelerating Transformer Inference for Translation via Parallel Decoding.
	
	`survey`: A survey on non autoregressive generation for neural machine translation and beyond.

- æ¨æµ‹æ€§è§£ç 

	`idea`: æ¨æµ‹æ€§æ‰§è¡Œæ¥åº”å¯¹é¡ºåºæ‰§è¡Œçš„é™åˆ¶ï¼Œå¹¶æé«˜è§£ç çš„å¹¶è¡Œæ€§ã€‚åœ¨è‡ªå›å½’LLMæ¨ç†è¿‡ç¨‹ä¸­çš„æ¯ä¸ªè§£ç æ­¥éª¤éƒ½å¯ä»¥è¢«è§†ä¸ºæ‰§è¡Œä¸€ä¸ªå¸¦æœ‰æ¡ä»¶åˆ†æ”¯çš„ç¨‹åºã€‚è¿™ä¸ªæ–¹æ³•æ¥è‡ªäºè¿™æ ·ä¸€ä¸ªäº‹å®ï¼šé¢„æµ‹çš„è¾“å‡ºæ€»æ˜¯ç”±åŸå§‹LLMéªŒè¯ï¼Œè€Œå½“é¢„æµ‹å‡ºé”™æ—¶ï¼Œå›é€€æœºåˆ¶ä¼šç”Ÿæ•ˆã€‚
	
	Speculative computation, parallelism, and functional programming.
	
	Accelerating large language model decoding with speculative sampling.
	
	Fast inference from transformers via speculative decoding.
	
	`idea`: é€šè¿‡å¼•å…¥å¤šä¸ªå°å‹è‰ç¨¿æ¨¡å‹ï¼Œå¹¶ç»“åˆä¸€ç§æ–°é¢–çš„åŸºäºæ ‘çš„æ¨æµ‹æ€§æ¨ç†å’ŒtokenéªŒè¯æœºåˆ¶ã€‚
	
	SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.
	
	å›é€€æœºåˆ¶ï¼šBig little transformer decoder

- æ—©æœŸé€€å‡ºæœºåˆ¶

	`idea`: åŸºäºæ—©æœŸæ¨¡å‹å±‚çš„è¾“å‡ºæœ‰å¯èƒ½è‡ªä¿¡åœ°æ¨æ–­å‡ºç›®æ ‡åˆ†å¸ƒã€‚å®ƒä»¬å¯ä»¥åŸºäºå†…éƒ¨åˆ†ç±»å™¨å‘å‡ºé¢„æµ‹ï¼Œè€Œä¸æ˜¯è¿è¡Œæ•´ä¸ªLLMã€‚
	
	Fast inference via early exiting from deep neural networks.
	
	Magic pyramid: Accelerating inference with early exiting and token pruning.
	
	Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting.
	
	A global past-future early exit method for accelerating inference of pre-trained language models.
	
	FastBERT: a Self-distilling BERT with Adaptive Inference Time.
	
	A simple hash-based early exiting approach for language understanding and generation.
	
	DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference.
	
	TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference.
	
	Learning to Skip for Language Modeling.
	
	Bert loses patience: Fast and robust inference with early exit.
	
	SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference.
	
	Consistent Accelerated Inference via Confident Adaptive Transformers.

- çº§è”æ¨ç†

	`idea`: çº§è”æ¨ç†é‡‡ç”¨ä¸€ç³»åˆ—è§„æ¨¡ä¸åŒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ¥æœ€å°åŒ–å“åº”æ—¶é—´ã€‚å°†å®ƒä»¬ä»¥çº§è”æ–¹å¼ç»„ç»‡èµ·æ¥ï¼Œå¹¶æ ¹æ®å®ä¾‹éš¾åº¦è‡ªé€‚åº”åœ°é€‰æ‹©åˆé€‚çš„åˆ†ç±»å™¨ã€‚
	
	Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade.
	
	Tabi: An Efficient Multi-Level Inference System for Large Language Models.
	
	FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance.
	
	On Optimal Caching and Model Multiplexing for Large Model Inference.
	
	LARGE LANGUAGE MODEL CASCADES WITH MIXTURE OF THOUGHT REPRESENTATIONS FOR COST-EFFICIENT REASONING.
	
	Chain-of-thought prompting elicits reasoning in large language models.
	
	Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.

#### Architecture Design
- é…ç½®ç¼©å‡ï¼š
	
	`idea`: ç¼©å‡æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨æµ…å±‚ç¼–ç å™¨æˆ–è§£ç å™¨
	
	PoWER-BERT: Accelerating BERT inference via progressive word-vector elimination.
	
	Adapler: Speeding up inference by adaptive length reduction.
	
	`idea`: æƒé‡å…±äº«ï¼Œè¯æ±‡è¡¨ç¼©å‡
	
	Shallow Decoder: Reevaluating Non-autoregressive Machine Translation.
- æ³¨æ„åŠ›ç®€åŒ–ï¼š

	è‡ªæ³¨æ„åŠ›è®¡ç®—çš„ä¸€ä¸ªçªå‡ºæŒ‘æˆ˜æ˜¯è®¡ç®—å¤æ‚åº¦$O(ğ¿^2)$ï¼Œå®ƒä¸è¾“å…¥åºåˆ—é•¿åº¦$ğ¿$æˆäºŒæ¬¡æ–¹å…³ç³»ã€‚ä¸ºäº†åº”å¯¹éå¸¸é•¿çš„åºåˆ—ä»»åŠ¡ï¼Œéœ€è¦å°†è¿™äº›æ ‡å‡†æ³¨æ„åŠ›ç®€åŒ–ä¸ºæ›´é«˜æ•ˆçš„é€‰æ‹©ã€‚
	
	`survey` : Efficient Transformers: A Survey.
	
	Big bird: Transformers for longer sequences.
	
	Transformers are rnns: Fast autoregressive transformers with linear attention.
	
	Linformer: Self-attention with linear complexity.
	
	`idea`: å€Ÿé‰´å…ˆå‰çš„æ³¨æ„åŠ›ç®€åŒ–æ–¹æ³•ï¼Œå°†å®ƒä»¬æ¦‚æ‹¬å’Œç»“åˆï¼Œä»¥ç¼©çŸ­ä¸Šä¸‹æ–‡å¹¶å‡å°‘KVç¼“å­˜çš„å¤§å°ï¼Œä»¥åŠé™ä½æ³¨æ„åŠ›å¤æ‚åº¦
	
	Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer.
	
	Mistral 7B.
	
	Faster Causal Attention Over Large Sequences Through Sparse Flash Attention.
	
	Longnet: Scaling transformers to 1,000,000,000 tokens.
	
	`idea`: é€šè¿‡å°†ä¸Šä¸‹æ–‡å‹ç¼©æˆæ›´å°‘çš„è½¯tokenæ¥è¿›è¡Œä¸Šä¸‹æ–‡å‹ç¼©
	
	Adapting Language Models to Compress Contexts.
	
	Landmark Attention: Random-Access Infinite Context Length for Transformers.
	
	In-context autoencoder for context compression in a large language model.
	
	CacheGen: Fast Context Loading for Language Model Applications.
	
	`idea`: æ ¹æ®ä¸åŒçš„é‡è¦æ€§æŒ‡å¯¼ç›´æ¥ä¸¢å¼ƒæˆ–é‡æ–°è¡¨è¿°ä¸é‡è¦çš„ä¸Šä¸‹æ–‡token
	
	Extending Context Window of Large Language Models via Semantic Compression.
	
	Llmlingua: Compressing prompts for accelerated inference of large language models.
	
	Compressing Context to Enhance Inference Efficiency of Large Language Models.
	
	Learning to compress prompts with gist tokens.
	
	Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers.
	
	Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time.
	
	H\_2 O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.
	
	Efficient Streaming Language Models with Attention Sinks.
	
	Longformer: The long-document transformer.

è¡¨1å±•ç¤ºäº†å››ç§ä»£è¡¨æ€§æ–¹æ³•çš„ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼åŠå…¶åº”ç”¨ã€‚ç„¶è€Œï¼Œç”±äºä¸Šä¸‹æ–‡ä¸å®Œæ•´ï¼Œè¿™äº›æ–¹æ³•åœ¨å®é™…å·¥ä½œè´Ÿè½½ä¸­å¯èƒ½é¢ä¸´ä¸å¯é¿å…çš„ä¿¡æ¯æŸå¤±ï¼Œç‰¹åˆ«æ˜¯å½“æ³¨æ„åŠ›åˆ†å¸ƒæ›´å¤æ‚æ—¶ã€‚
![](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/image.5q7eh98kv4.webp)

- æ¿€æ´»å…±äº«ï¼šé€šè¿‡å…±äº«ä¸­é—´æ¿€æ´»æ¥æé«˜æ³¨æ„åŠ›è®¡ç®—çš„æ•ˆç‡ã€‚
	
	`idea`: æ³¨æ„åŠ›å…±äº«æ–¹æ³•è§‚å¯Ÿåˆ°ä¸åŒå±‚çš„æ³¨æ„åŠ›çŸ©é˜µåˆ†å¸ƒä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œå¹¶é‡æ–°ä½¿ç”¨è¿™äº›æ³¨æ„åŠ›çŸ©é˜µä»¥å‡å°‘è®¡ç®—æˆæœ¬ã€‚
	
	An efficient transformer decoder with compressed sub-layers.
	
	Speeding up Transformer Decoding via an Attention Refinement Network.
	
	Sharing Attention Weights for Fast Transformer.
	
	`idea`: å¤šæŸ¥è¯¢æ³¨æ„åŠ›(MQA)ä½¿å¾—ä¸åŒçš„å¤´å…±äº«åŒä¸€ç»„é”®å’Œå€¼ï¼Œä»¥å‡å°‘å¢é‡æ¨ç†ä¸­çš„å†…å­˜å¸¦å®½éœ€æ±‚ã€‚
	
	Fast transformer decoding: One write-head is all you need.
	
	`idea`: ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)æ”¾å®½äº†å•ä¸€ç»„é”®å’Œå€¼çš„é™åˆ¶åˆ°å¤šç»„ï¼Œå¹¶ä¸”æ¯ç»„ä¸ä¸€ç»„æŸ¥è¯¢è€¦åˆã€‚
	
	GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.
	
- æ¡ä»¶è®¡ç®—ï¼š
	
	ç¨€ç–æ¿€æ´»çš„ä¸“å®¶æ··åˆæ¨¡å‹ï¼ˆMoEï¼‰èŒƒå¼å°†æ¨¡å‹çš„èƒ½åŠ›åˆ†å¸ƒåœ¨å„ç§â€œä¸“å®¶â€ä¸Šï¼Œè¿™äº›â€œä¸“å®¶â€æ˜¯æ›´å°çš„ç¥ç»ç½‘ç»œï¼Œæ¯ä¸ªä¸“å®¶ä¸“æ³¨äºæ•°æ®çš„ä¸åŒå­é›†ã€‚å®ƒå…è®¸ç³»ç»Ÿä»…æ ¹æ®æŸäº›è·¯ç”±æœºåˆ¶è°ƒç”¨ç»™å®šè¾“å…¥æ‰€éœ€çš„ä¸“å®¶ï¼Œè€Œä¸æ˜¯åœ¨æ•´ä¸ªå¤§å‹æ¨¡å‹ä¸Šè®¡ç®—ï¼Œä»è€Œå®ç°äº†è®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚
	
	SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention.
	
	Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
	
	Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
	
	GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.
	
	Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate.
	
	Hash layers for large sparse models.
	
	Memory Augmented Language Models through Mixture of Word Experts.
	
	Mixture-of-experts with expert choice routing.
	
	Glam: Efficient scaling of language models with mixture-of-experts.
	
	Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference.
	
	MoEçš„åŠ¨æ€ç‰¹æ€§ä¹Ÿè¦æ±‚ç‰¹æ®Šçš„ç³»ç»Ÿä¼˜åŒ–ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼é€šä¿¡GPUå†…æ ¸å®ç°ï¼Œä»¥ä¿ƒè¿›MoEæ¨ç†æ•ˆç‡ã€‚
	
	FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models.
	
	Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference.
	
	Tutel: Adaptive mixture-of-experts at scale.
	
	Accelerating Distributed {MoE} Training and Inference with Lina.
	
	FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement.
	
	Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale.
	
	MegaBlocks: Efficient Sparse Training with Mixture-of-Experts.
	
	PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation.

- å¾ªç¯å•å…ƒ
	
	å°½ç®¡é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰åœ¨æ•æ‰åºåˆ—ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»æ–¹é¢å­˜åœ¨å›°éš¾ï¼Œä½†ä»æœ‰å‡ ç§æ–¹æ³•ä½¿ç”¨é€’å½’å•å…ƒæ¥æ›¿æ¢Transformeræ¨¡å—ï¼Œå¹¶åœ¨æ¨ç†æœŸé—´å®ç°çº¿æ€§çš„è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦ã€‚
	
	RWKV: Reinventing RNNs for the Transformer Era.
	
	Retentive Network: A Successor to Transformer for Large Language Models.
	
	`idea`: è¿™äº›æœ€è¿‘çš„æ¢ç´¢å¤§å¤šå»ºç«‹åœ¨çº¿æ€§æ³¨æ„åŠ›è¡¨ç¤ºä¹‹ä¸Šã€‚ç»è¿‡é‡ç»„åï¼Œå®ƒä»¬é€šè¿‡ä½¿ç”¨çº¿æ€§é€’å½’å•å…ƒå¯¹tokenä¹‹é—´çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ï¼Œä»è€Œå…‹æœäº†æ³¨æ„åŠ›çš„$O(L^2)$ç“¶é¢ˆï¼Œè¿™äº›é€’å½’å•å…ƒæ›´å®¹æ˜“ä¿æŒå¯å¹¶è¡ŒåŒ–è®­ç»ƒçš„æ€§è´¨ã€‚
	
	Transformers are rnns: Fast autoregressive transformers with linear attention.
	
	An attention free transformer.
	
	Hungry Hungry Hippos: Towards Language Modeling with State Space Models.
	
	Mamba: Linear-Time Sequence Modeling with Selective State Spaces.
	
	Efficiently Modeling Long Sequences with Structured State Spaces.
	
	Long Range Language Modeling via Gated State Spaces.
	
	Resurrecting recurrent neural networks for long sequences.
	
	`idea`: è®¾è®¡è¿˜åŒ…æ‹¬å„ç§ä½ç½®ç¼–ç æ¨¡å—ï¼ŒæŒ‡æ•°è¡°å‡æœºåˆ¶ä»¥åŠä¸€ç³»åˆ—tokençº§åˆ«çš„éçº¿æ€§MLPsæˆ–GLUsï¼Œä»¥æ”¹è¿›æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
	
	Roformer: Enhanced transformer with rotary position embedding.
	
	The statistical recurrent unit.
	
	Mlp-mixer: An all-mlp architecture for vision.
	
	Metaformer is actually what you need for vision.
	
	Language modeling with gated convolutional networks.

#### Model Compression

- çŸ¥è¯†è’¸é¦ï¼š
	
	`idea`: é€šè¿‡å¤§å‹æ•™å¸ˆæ¨¡å‹çš„ç›‘ç£æ¥è®­ç»ƒä¸€ä¸ªå°å‹å­¦ç”Ÿæ¨¡å‹ã€‚å¤§å¤šæ•°å…ˆå‰æ–¹æ³•éƒ½åœ¨æ¢ç´¢ç™½ç›’è’¸é¦ï¼Œè¿™éœ€è¦è®¿é—®æ•´ä¸ªæ•™å¸ˆæ¨¡å‹çš„å‚æ•°ã€‚
	
	Knowledge Distillation of Large Language Models.
	
	TinyBERT: Distilling BERT for Natural Language Understanding.
	
	DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter.
	
	Patient Knowledge Distillation for BERT Model Compression.
	
	Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers.
	
	`idea`: é»‘ç›’è’¸é¦
	
	Stanford alpaca: An instruction-following llama model.
	
	Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.
	
	Wizardlm: Empowering large language models to follow complex instructions.
	
	Instruction tuning with gpt-4.
	
	Minigpt-4: Enhancing vision-language understanding with advanced large language models.
	
	OpenAI. 2023. GPT-4 Technical Report.
	
- ç½‘ç»œå‰ªæ: 
	
	ç»“æ„åŒ–å‰ªææ–¹æ³•ï¼Œè¿™ç§»é™¤äº†æ•´ä¸ªç»“æ„åŒ–çš„LLMç»„ä»¶ï¼Œä¾¿äºGPUåŠ é€Ÿã€‚
	
	Reducing Transformer Depth on Demand with Structured Dropout.
	
	Ziplm: Hardware-aware structured pruning of language models.
	
	LLM-Pruner: On the Structural Pruning of Large Language Models.
	
	What Matters In The Structured Pruning of Generative Language Models?
	
	Deja vu: Contextual sparsity for efficient llms at inference time.
	
	éç»“æ„åŒ–æ–¹æ³•ï¼šå®ƒä»¬é€šå¸¸å®ç°50-60%çš„ç¨€ç–åº¦ä»¥å‹ç¼©LLMsã€‚å¯ä»¥è¿›ä¸€æ­¥æ³›åŒ–åˆ°åŠç»“æ„åŒ–çš„N:Mç¨€ç–ï¼ˆå³2:4å’Œ4:8ï¼‰ï¼Œåˆ©ç”¨NVIDIAç¨€ç–å¼ é‡æ ¸å¿ƒçš„åŠ é€Ÿï¼Œæ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ã€‚
	
	Accelerating sparse deep neural networks.
	
	LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation.
	
	DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization.
	
	Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity.
	
	PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU.

### System Optimization

#### Low-bit Quantizationï¼š

`idea`:é€šè¿‡ä½¿ç”¨æ›´å°‘çš„æ¯”ç‰¹ï¼ˆå³å°‘äº32æ¯”ç‰¹ï¼‰æ¥è¡¨ç¤ºæ•°å€¼ï¼Œä¸€ç§æ–¹æ³•æ˜¯å¯¹LLMè¿›è¡Œé‡åŒ–ã€‚

`survey`: A comprehensive study on post-training quantization for large language models.

`idea`: é‡åŒ–æ„ŸçŸ¥è®­ç»ƒä¸è®­ç»ƒåé‡åŒ–ï¼ŒPTQé€šè¿‡ä½¿ç”¨è‡ªå®šä¹‰çš„CUDAå†…æ ¸æˆ–ç¼–è¯‘å°†æ¨¡å‹æƒé‡çš„è®¡ç®—ç²¾åº¦ç”šè‡³æ¿€æ´»å€¼é™ä½åˆ°INT8æˆ–INT4ã€‚

A Speed Odyssey for Deployable Quantization of LLMs.

nuqmm: Quantized matmul for efficient inference of large-scale generative language models.

Atom: Low-bit Quantization for Efficient and Accurate LLM Serving.

LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.

SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression.

Gptq: Accurate post-training quantization for generative pre-trained transformers.

OPTQ: Accurate quantization for generative pre-trained transformers.

AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration.

Smoothquant: Accurate and efficient post-training quantization for large language models.

Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.

RPTQ: Reorder-based Post-training Quantization for Large Language Models.

UnderstandingINT4Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases.

SqueezeLLM: Dense-and-Sparse Quantization.

Qlora: Efficient finetuning of quantized llms.

LLM-QAT: Data-Free Quantization Aware Training for Large Language Models.

The case for 4-bit precision: k-bit Inference Scaling Laws.

CacheGen: Fast Context Loading for Language Model Applications.

Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization.

#### Parallel Computation.

- æ¨¡å‹å¹¶è¡Œï¼š
	
	`idea`: å¼ é‡æ¨¡å‹å¹¶è¡Œå°†æ¨¡å‹å±‚ï¼ˆä¾‹å¦‚ï¼Œæ³¨æ„åŠ›ã€FFNï¼‰ä»å†…éƒ¨ç»´åº¦ï¼ˆä¾‹å¦‚ï¼Œå¤´ã€éšè—å±‚ï¼‰åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†ï¼Œå¹¶åœ¨å•ç‹¬çš„è®¾å¤‡ï¼ˆä¾‹å¦‚ï¼ŒGPUï¼‰ä¸Šéƒ¨ç½²æ¯ä¸ªéƒ¨åˆ†ã€‚
	
	Megatron-lm: Training multi-billion parameter language models using model parallelism.
	
	Efficiently scaling transformer inference.
	
	SUMMA: Scalable universal matrix multiplication algorithm.
	
	`idea`: ç®¡é“æ¨¡å‹å¹¶è¡Œå°†æ¨¡å‹å±‚æŒ‰é¡ºåºæ’åˆ—åœ¨å¤šä¸ªè®¾å¤‡ä¸Šã€‚æ¯ä¸ªè®¾å¤‡è´Ÿè´£ä¸€ä¸ªç®¡é“é˜¶æ®µï¼Œè¯¥é˜¶æ®µç”±å¤šä¸ªè¿ç»­çš„æ¨¡å‹å±‚ç»„æˆã€‚
	
	Memory-efficient pipeline parallel dnn training.
	
	`idea`: åºåˆ—å¹¶è¡Œæœ‰å„ç§å·®å¼‚åŒ–çš„è®¾è®¡å’Œå®ç°ï¼Œä½†å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å¯¹é•¿åºåˆ—çš„å¤„ç†åœ¨å¤šä¸ªGPUä¹‹é—´è¿›è¡Œåˆ†å‰²ï¼Œä»è€Œåˆ†å¸ƒå¼è®¡ç®—å’Œå­˜å‚¨è´Ÿè½½ã€‚
	
	Ring Attention with Blockwise Transformers for Near-Infinite Context.
	
	Calculon: a methodology and tool for high-level co-design of systems and large language models.
	
	`idea`: è‡ªåŠ¨å¹¶è¡Œç”¨äºåˆ†å¸ƒå¼è®­ç»ƒï¼Œé€šè¿‡æ›¿æ¢å®ƒä»¬çš„æˆæœ¬æ¨¡å‹ä»¥é€‚åº”Transformeræ¨¡å‹çš„å¯é¢„æµ‹è¿è¡Œæ—¶ï¼Œå¯ä»¥è½»æ¾åœ°å°†å…ˆå‰çš„è‡ªåŠ¨æœç´¢ç®—æ³•ï¼ˆä¾‹å¦‚ï¼ŒåŠ¨æ€è§„åˆ’ï¼Œæ•´æ•°çº¿æ€§è§„åˆ’ï¼‰åº”ç”¨äºLLMæœåŠ¡ï¼Œå¹¶åœ¨æ— éœ€æ‰‹åŠ¨å¹²é¢„çš„æƒ…å†µä¸‹ç¡®å®šæœ€æœ‰æ•ˆçš„å¹¶è¡Œç­–ç•¥ã€‚
	
	Alpa: Automating inter-and {Intra-Operator} parallelism for distributed deep learning.
	
	Beyond Data and Model Parallelism for Deep Neural Networks.
	
	Unity: Accelerating {DNN} training through joint optimization of algebraic transformations and parallelization.
	
	Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism.
	
	Cheaply Estimating Inference Efficiency Metrics for Autoregressive Transformer Models.
	
	AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving.
	
	FlexFlow-Serve. https://github.com/Flexflow/FlexFlow/tree/inference. Commit: 672cdad, Accessed on: 2023-11 25.
	
	SpotServe: Serving Generative Large Language Models on Preemptible Instances.
	
	`idea`: ä½¿èƒ½å¸è½½æŠ€æœ¯ï¼Œé™¤äº†æœ‰é™çš„è®¾å¤‡å†…å­˜ï¼ˆä¾‹å¦‚ï¼ŒGPU DRAMï¼‰ä¹‹å¤–ï¼Œè¿˜ä½¿ç”¨æ›´å¤§ä½†æ›´æ…¢çš„å†…å­˜ï¼ˆä¾‹å¦‚ï¼ŒCPU DRAMï¼‰æ¥ä¿å­˜æ¨¡å‹å‚æ•°å’ŒKVç¼“å­˜ã€‚
	
	LLM in a flash: Efficient Large Language Model Inference with Limited Memory.
	
	Deep speed inference: Enabling efficient inference of transformer models at unprecedented scale.
	
	STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining.
	
	SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.
	
	FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU.
	
- å»ä¸­å¿ƒåŒ–æ¨ç†ï¼š
	
	`idea`: è¿™ç§æ–¹æ³•æ¶‰åŠæ¨¡å‹å’Œæ•°æ®å¹¶è¡Œä¸»ä¹‰çš„ç»“åˆï¼Œå…¶ä¸­å¤šä¸ªå»ä¸­å¿ƒåŒ–çš„è‡ªæ„¿èŠ‚ç‚¹åä½œå¤„ç†æ•°æ®å¹¶æ¨æ–­è¾“å‡ºã€‚è¿™ç§æ–¹æ³•åœ¨ç¡¬ä»¶èµ„æºåœ°ç†åˆ†å¸ƒçš„åœºæ™¯ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚
	
	Petals: Collaborative inference and fine-tuning of large models.
	
	HexGen: Generative Inference of Foundation Model over Heterogeneous Decentralized Environment.
	
	Distributed Inference and Fine-tuning of Large Language Models Over The Internet.
	
	FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs.

#### Memory Management

é«˜æ•ˆçš„å†…å­˜ç®¡ç†ä»ç„¶æ˜¯LLMæœåŠ¡ä¸­çš„é¦–è¦æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯è€ƒè™‘åˆ°å˜å‹å™¨æ¶æ„å›ºæœ‰çš„å†…å­˜å¯†é›†å‹ç‰¹æ€§ã€‚éšç€å¯¹é•¿åºåˆ—æ¨ç†éœ€æ±‚çš„å¢é•¿ï¼ŒKVç¼“å­˜çš„å†…å­˜å ç”¨æˆä¸ºäº†ç›¸æ¯”äºæ¨¡å‹æƒé‡å’Œå…¶ä»–æ¿€æ´»æ‰€éœ€å·¥ä½œç©ºé—´çš„ä¸»è¦ä¼˜åŒ–ç›®æ ‡ã€‚

Efficient Memory Management for Large Language Model Serving with PagedAttention.

SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.

LightLLM. https://github.com/ModelTC/lightllm.

æ˜¾è€Œæ˜“è§ï¼ŒLLMæ¨ç†ä¸­çš„å†…å­˜å‡å°‘ä¸å…¶å®ƒç®—æ³•åˆ›æ–°å’Œç³»ç»Ÿçº§ä¼˜åŒ–ç´§å¯†ç›¸å…³ã€‚è™½ç„¶æŸäº›æ–¹æ³•å¯èƒ½é€‚ç”¨äºç‰¹å®šçš„å·¥ä½œè´Ÿè½½ï¼Œä½†å®ƒä»¬å¯èƒ½ä¼šç›¸äº’æŠµæ¶ˆï¼Œå¯¼è‡´æ•´ä½“æ€§èƒ½ä¸‹é™ã€‚åœ¨LLMæ¨ç†ç³»ç»Ÿçš„å†…å­˜æ•ˆç‡å’Œè®¡ç®—æ€§èƒ½ä¹‹é—´æ‰¾åˆ°æ­£ç¡®çš„å¹³è¡¡ä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾ä¸”ç´§è¿«çš„æŒ‘æˆ˜ã€‚

#### Request Scheduling

æœ‰æ•ˆåœ°è°ƒåº¦ä¼ å…¥çš„æ¨ç†è¯·æ±‚å¯¹äºä¼˜åŒ–LLMæœåŠ¡è‡³å…³é‡è¦ï¼Œè¿™äº›ç®—æ³•æ—¨åœ¨æœ€å¤§åŒ–èµ„æºåˆ©ç”¨ç‡ï¼Œä¿è¯åœ¨å»¶è¿ŸæœåŠ¡æ°´å¹³ç›®æ ‡ï¼ˆSLOï¼‰å†…çš„å“åº”æ—¶é—´ï¼Œå¹¶æœ‰æ•ˆå¤„ç†å˜åŒ–çš„éœ€æ±‚è´Ÿè½½ã€‚é«˜æ•ˆåœ°ç®¡ç†ä¼ å…¥è¯·æ±‚å¹¶ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡ã€‚

Batch: machine learning inference serving on serverless platforms with adaptive batching.

Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences.

Paella: Low-latency Model Serving with Software defined GPU Scheduling.

PipeSwitch: Fast pipelined context switching for deep learning applications.

Cocktail: A multidimensional optimization for model serving in cloud.

MArk: Exploiting Cloud Services for  Machine Learning Inference Serving.

è€ƒè™‘åˆ°å¯å˜çš„è¾“å‡ºåºåˆ—é•¿åº¦ï¼Œå®ƒä»¥é¦–æ¬¡åˆ°è¾¾ä¼˜å…ˆï¼ˆFCFSï¼‰çš„é¡ºåºåœ¨è¿­ä»£çº§åˆ«è°ƒåº¦å¼•æ“çš„æ‰§è¡Œï¼Œå¹¶å…è®¸å¯¹é€‰å®šçš„æ“ä½œé›†è¿›è¡Œæ‰¹å¤„ç†ä»¥æ›´å¥½åœ°åˆ©ç”¨ç¡¬ä»¶ã€‚

Orca: A Distributed Serving System for Transformer-Based Generative Models.

RayLLM. https://github.com/ray-project/ray-llm.

NVIDIA TensorRT-LLM. https://github.com/NVIDIA/TensorRT-LLM.

Fast Distributed Inference Serving for Large Language Models.

SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills.

DeepSpeed-FastGen. https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen.

S3: Increasing GPU Utilization during Generative Inference for Higher Throughput.

#### Kernel Optimization

- å†…æ ¸èåˆï¼š
	
	`idea`: ä¸ºäº†å‡å°‘å†…æ ¸å¯åŠ¨å’Œå†…å­˜è®¿é—®çš„å¼€é”€ï¼Œå†…æ ¸èåˆè¢«ä¹‹å‰çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¡†æ¶å’Œç¼–è¯‘å™¨å¹¿æ³›é‡‡ç”¨ã€‚ç”±äºLLMæ¨ç†ä¸éœ€è¦åå‘è®¡ç®—ï¼Œå› æ­¤å­˜åœ¨æ›´å¤šçš„å†…æ ¸èåˆæœºä¼šã€‚
	
	NVIDIA Faster Transformer. https://github.com/NVIDIA/FasterTransformer.
	
	TenTrans High-Performance Inference Toolkit for WMT2021 Efficiency Task.
	
	Turbotransformers: an efficient gpu serving system for transformer models.
	
	LightSeq: A high performance inference library for transformers.
	
	A high-performance transformer boosted for variable-length inputs.
	
	Welder: Scheduling Deep Learning Memory Access via Tile-graph.
	
- å®šåˆ¶åŒ–æ³¨æ„åŠ›ï¼š
	
	`idea`: ä¸ºäº†ä½¿æ³¨æ„åŠ›æ“ä½œåœ¨GPUä¸Šé«˜æ•ˆè¿è¡Œï¼Œä¸“é—¨ä¸ºæ³¨æ„åŠ›è®¡ç®—å®šåˆ¶GPUå†…æ ¸æ˜¯è‡³å…³é‡è¦çš„ã€‚
	
	NVIDIA cuDNN MultiHeadAttn. https://docs.nvidia.com/deeplearning/cudnn/api/index.html# cudnnMultiHeadAttnForward.
	
	`idea`: ç”¨äºç¬¬ä¸€æ¬¡è¿­ä»£ï¼ˆå³åˆå§‹/é¢„å¡«å……/ä¸Šä¸‹æ–‡/æç¤ºé˜¶æ®µï¼‰ï¼Œå®ƒå¹¶è¡Œå¤„ç†è¾“å…¥æç¤ºä¸­çš„æ‰€æœ‰tokenã€‚
	
	xFormers: A modular and hackable Transformer modelling library. https://github.com/facebookresearch/xformers.
	
	NVIDIA CUTLASS. https://github.com/NVIDIA/cutlass.
	
	Accelerating transformer networks through recomposing softmax layers.
	
	Online normalizer calculation for softmax.
	
	Self-attention Does Not Need $O(ğ‘›^2)$ Memory.
	
	ç”¨äºåç»­è¿­ä»£ï¼ˆå³å¢é‡/è§£ç /ç”Ÿæˆé˜¶æ®µï¼‰ï¼Œæ¯ä¸ªè¿­ä»£åªç”Ÿæˆä¸€ä¸ªè¾“å‡ºtokençš„å†…æ ¸ã€‚
	
	`idea`: å¯¹äºè‡ªå›å½’è§£ç ï¼Œå¸¸è§çš„åšæ³•æ˜¯ä¿å­˜å…ˆå‰è®¡ç®—è¿‡çš„é”®å’Œå€¼ï¼Œè¿™æ ·åœ¨ç”Ÿæˆæ–°ä»¤ç‰Œæ—¶åªéœ€è¦è®¡ç®—ä¸€ä¸ªæŸ¥è¯¢ï¼Œè€Œä¸æ˜¯é‡æ–°è¿è¡Œæ•´ä¸ªåºåˆ—ã€‚è¿™ä¸ªé¢†åŸŸä¼˜åŒ–çš„ä¸»è¦æ–¹å‘æ˜¯æœ€å¤§åŒ–çº¿ç¨‹å ç”¨ç‡å¹¶æœ€å°åŒ–è®¾å¤‡ä¸Šçš„é«˜å¸¦å®½å†…å­˜ã€‚
	
	Et: re-thinking self-attention for transformer models on gpus.
	
	Flash-Decoding for long-context inference.
	
	FlashDecoding++: Faster Large Language Model Inference on GPUs.
	
	æ ¹æ®å·¥ä½œè´Ÿè½½é€‰æ‹©åˆé€‚çš„å¹¶è¡Œç»´åº¦å¯¹äºæ›´å¥½çš„çº¿ç¨‹åˆ©ç”¨ç‡æ˜¯å¿…è¦çš„ã€‚
	
- é‡‡æ ·ä¼˜åŒ–ï¼š
	
	å¹¶è¡Œé‡‡æ ·æŠ€æœ¯ï¼Œå¦‚æŸæœç´¢ï¼ˆbeam searchï¼‰ï¼Œé€šè¿‡åœ¨æ¯æ¬¡è¿­ä»£ä¸­ç»´æŠ¤å›ºå®šæ•°é‡ï¼ˆå³æŸå®½ï¼‰çš„æœ€é«˜åˆ†åºåˆ—ï¼Œæœ‰æ•ˆåœ°è§£ç è¿‘ä¼¼æœ€ä¼˜åºåˆ—
	
	`idea`: æå‡ºäº†å¤šç§éšæœºé‡‡æ ·æŠ€æœ¯æ¥å¼•å…¥éšæœºæ€§ï¼Œä»¥è·å¾—æ›´å¤šæ ·åŒ–çš„è¾“å‡ºã€‚
	
	Hierarchical Neural Story Generation.
	
	The curious case of neural text degeneration.
	
	Ctrl: A conditional transformer language model for controllable generation.
	
	`idea`: ç”±äºå†—ä½™çš„KVç¼“å­˜å¯¼è‡´çš„å†…å­˜å‹åŠ›å¢åŠ ï¼Œå¹¶ä¸”LLMçš„å¤§è¯æ±‡é‡ï¼ˆå³æ•°ä»¥ä¸‡è®¡ï¼‰å¯¼è‡´çš„é‡‡æ ·æ•ˆç‡é—®é¢˜ã€‚
	
	LightSeq: A high performance inference library for transformers.
	
- å¯å˜åºåˆ—é•¿åº¦ï¼š
	
	`idea`: LLMæ¨ç†çš„å¦ä¸€ä¸ªç‹¬ç‰¹æŒ‘æˆ˜æ˜¯åºåˆ—åœ¨è¾“å…¥é•¿åº¦å’Œè¾“å‡ºé•¿åº¦ä¸Šå¯ä»¥å˜åŒ–ï¼Œä¸”åè€…æ˜¯é¢„å…ˆæœªçŸ¥çš„ã€‚ä¸€ç§åŠ å¿«æ¨ç†é€Ÿåº¦çš„æ–¹æ³•æ˜¯ä¸€æ¬¡å¤„ç†å¤šä¸ªåºåˆ—çš„æ‰¹æ¬¡ã€‚ç„¶è€Œï¼Œå½“ä¸€æ‰¹åºåˆ—å…·æœ‰å¯å˜çš„è¾“å…¥é•¿åº¦æ—¶ï¼Œé€šå¸¸ä¼šä½¿ç”¨å¡«å……ï¼ˆpaddingï¼‰æ¥ä½¿å®ƒä»¬åœ¨æ‰¹é‡å¤„ç†æ—¶é•¿åº¦ç›¸åŒï¼Œè¿™æ ·åšæµªè´¹äº†è®¡ç®—å’Œå†…å­˜èµ„æºã€‚
	
	NVIDIA Effective Transformer. https://github.com/bytedance/effective_transformer.
	
	Bytetransformer: A high-performance transformer boosted for variable-length inputs.
	
	The CoRa tensor compiler: Compilation for ragged tensors with minimal padding.
	
	Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs.
	
	SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills.
	
- è‡ªåŠ¨ç¼–è¯‘
	
	å¤§å¤šæ•°ç°æœ‰çš„LLMæ¨ç†ç³»ç»Ÿä½¿ç”¨ç‰¹å®šä¾›åº”å•†çš„åº“ä½œä¸ºå…¶åç«¯ï¼Œä¾‹å¦‚cuBLASã€cuDNNå’ŒCUTLASSï¼Œè¿™äº›åº“æä¾›äº†ä¼˜åŒ–çš„å†…æ ¸å®ç°ã€‚ä¸ºäº†è¿›ä¸€æ­¥æé«˜æ¨ç†æ•ˆç‡ï¼Œå®ƒä»¬è¿˜ä»˜å‡ºäº†å·¨å¤§åŠªåŠ›æ¥ä¸ºç‰¹å®šçš„LLMè¿ç®—ç¬¦ï¼ˆä¾‹å¦‚ï¼Œæ³¨æ„åŠ›ï¼‰åœ¨NVIDIA GPUä¸Šæ‰‹åŠ¨ç¼–å†™ä¼˜åŒ–çš„å†…æ ¸ã€‚å°½ç®¡æœ‰è¿™äº›å·¥ä½œï¼Œä½¿ç”¨è‡ªåŠ¨åŒ–DNNç¼–è¯‘å™¨çš„è¶‹åŠ¿ä»ç„¶å­˜åœ¨ã€‚
	
	Apache TVM Unity: a vision for the ML software and hardware ecosystem.
	
	Relax: Composable Abstractions for End-to-End Dynamic Machine Learning.
	
	Tensorir: An abstraction for automatic tensorized program optimization.
	
	SparseTIR: Composable abstractions for sparse compilation in deep learning.
	
	MLIR-based code generation for GPU tensor cores.
	
	Compiling machine learning programs via high-level tracing.
	
	Triton: an intermediate language and compiler for tiled neural network computations.
	
	TASO: optimiz ing deep learning computation with automatic generation of graph substitutions.
	
	PyTorch 2.0: The Journey to Bringing Compiler Technologies to the Core of PyTorch.
	
	EINNET: Optimizing Tensor Programs with Derivation-Based Transformations.

## Software Frameworks

![Comparison of state-of-the-art open-sourced GPU-based LLM serving systems.](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/paper/image.2krwjy3ire.webp)

## Benchmarks



## Connection with other surveys

æˆ‘ä»¬çš„è°ƒç ”åœ¨é«˜æ•ˆç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡å’Œæ¨ç†æ–¹é¢ï¼Œè¡¥å……å¹¶æ‹“å±•äº†ç°æœ‰é¢†åŸŸæ–‡çŒ®çš„èŒƒå›´ï¼ŒåŒæ—¶ä¿æŒäº†ç‹¬ç‰¹çš„å…³æ³¨ç‚¹ã€‚åœ¨ç›¸å…³å·¥ä½œä¸­ï¼Œ[144]çš„ç ”ç©¶ä¸»é¢˜ä¸æˆ‘ä»¬çš„è°ƒç ”æœ€ä¸ºæ¥è¿‘ï¼Œå®ƒæ¢è®¨äº†æ›´é€šç”¨çš„Transformeræ¨¡å‹å’Œç‰¹å®šé¢†åŸŸçš„åŠ é€Ÿå™¨è®¾è®¡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„è°ƒç ”é€šè¿‡ä¸“é—¨é’ˆå¯¹ç”Ÿæˆå¼LLMæœåŠ¡è¿™ä¸€ç²¾ç»†é¢†åŸŸï¼Œä¸å…¶ä»–ç ”ç©¶åŒºåˆ†å¼€æ¥ï¼Œè¿™ä¸€é¢†åŸŸå°šæœªæˆä¸ºå…¶ä»–ç ”ç©¶çš„ä¸­å¿ƒã€‚æ­¤å¤–ï¼Œä¸€äº›ç ”ç©¶æ·±å…¥è¿›è¡Œäº†LLMåœ¨GPUä¸Šæ¨ç†æ•ˆç‡[190, 297]å’Œæ–°å‹åŠ é€Ÿå™¨[78]çš„å®éªŒæ€§ç ”ç©¶ï¼Œæä¾›äº†ç›´æ¥å…³è”æˆ‘ä»¬æœåŠ¡æ•ˆç‡ç ”ç©¶çš„å®è´µå®è¯è§è§£ã€‚æ­¤å¤–ï¼ŒLLMCarbon [79] å…³æ³¨äº†LLMéƒ¨ç½²ä¸­è¶Šæ¥è¶Šé‡è¦çš„ä¸€ä¸ªæ–¹é¢â€”â€”å¯¹ç¯å¢ƒçš„å½±å“ï¼ˆä¾‹å¦‚ï¼Œç¢³è¶³è¿¹ï¼‰ã€‚å°½ç®¡æˆ‘ä»¬çš„è°ƒç ”ä¸»è¦ä»æ€§èƒ½è§’åº¦å…³æ³¨æ•ˆç‡ï¼Œä½†è¿™ç±»ç ”ç©¶æä¾›çš„ç¯å¢ƒè§†è§’æ— ç–‘åœ¨æˆ‘ä»¬çš„å¹¿æ³›è®¨è®ºä¸­æ˜¯ç›¸å…³ä¸”å€¼å¾—å°Šæ•¬çš„ã€‚ä¸€äº›è°ƒç ”å’ŒåŸºå‡†æµ‹è¯•[126]æä¾›äº†å…³äºæ¨¡å‹å‹ç¼©[113, 248, 314, 314]å’Œé‡åŒ–[99, 280]çš„å®è´µè§è§£ï¼Œè¿™äº›ç ”ç©¶ä¸ºæˆ‘ä»¬çš„ç›¸å…³æ–¹å‘æ¢ç´¢é—´æ¥æä¾›äº†åŸºç¡€ã€‚ä¸€äº›ç ”ç©¶[65, 187]ä¸ºç†è§£LLMçš„æœ‰æ•ˆæ€§ï¼ˆä¾‹å¦‚ï¼Œå‡†ç¡®æ€§ã€å›°æƒ‘åº¦ã€äº‹å®æ€§ç­‰ï¼‰æä¾›äº†å¿…è¦çš„èƒŒæ™¯ï¼Œè¿™è¶…å‡ºäº†æˆ‘ä»¬è°ƒç ”çš„èŒƒå›´ã€‚æˆ‘ä»¬çš„è°ƒç ”ä¹Ÿè®¤å¯äº†å…ˆå‰ä¸“æ³¨äºå¤§è§„æ¨¡æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰æ¨¡å‹åˆ†å¸ƒå¼è®­ç»ƒçš„è°ƒç ”[42, 175]çš„è´¡çŒ®ï¼Œå› ä¸ºå®ƒä»¬ä¸ºè€ƒè™‘LLMæœåŠ¡æä¾›äº†èƒŒæ™¯ä¿¡æ¯ã€‚ä»æœ¬è´¨ä¸Šè®²ï¼Œæˆ‘ä»¬çš„è°ƒç ”ä½äºä¼—å¤šç ”ç©¶ä¹‹ä¸­ï¼Œä»ä¸­å¸å–å¹¶è´¡çŒ®äº†å¯¹LLMæœåŠ¡æ•ˆç‡æ›´å…¨é¢çš„ç†è§£ï¼ŒåŒ…æ‹¬ç®—æ³•åˆ›æ–°å’Œç³»ç»Ÿä¼˜åŒ–ã€‚é€šè¿‡æ•´åˆè¿™äº›é¢†åŸŸçš„è§è§£ï¼Œæˆ‘ä»¬æ—¨åœ¨æä¾›ä¸€ä¸ªç»†è‡´è€Œå…¨é¢çš„æ¦‚è¿°ï¼Œæ¶µç›–è¯¥é¢†åŸŸæœ€æ–°çš„è¿›å±•å’ŒæŒ‘æˆ˜ã€‚

Full stack optimization of transformer inference: a survey.

Cheaply Estimating Inference Efficiency Metrics for Autoregressive Transformer Models.

Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models.

A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators.

LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models.

Compressing LLMs: The Truth is Rarely Pure and Never Simple.

Compression of deep learning models for text: A survey.

Efficient methods for natural language processing: A survey.

A survey on model compression for large language models.

A survey of quantization methods for efficient neural network inference.

A comprehensive study on post-training quantization for large language models.

LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking.

Generating benchmarks for factuality evaluation of language models.

Demystifying parallel and distributed deep learning: An in-depth concurrency analysis.

Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools.

## Future Direction

### ç¡¬ä»¶åŠ é€Ÿå™¨çš„å‘å±•ä¸å¢å¼º

æé«˜ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡æ•ˆç‡çš„æœªæ¥è¿›å±•ï¼Œå¯èƒ½ä¼šåœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºä¸“é—¨ç¡¬ä»¶åŠ é€Ÿå™¨çš„å¼€å‘å’Œå®Œå–„ï¼Œä»¥åŠä¸ç¡¬ä»¶å’Œè½¯ä»¶ä¼˜åŒ–ç›¸åè°ƒçš„å…±åŒè®¾è®¡æ–¹æ³•ã€‚

ä¾‹å¦‚ï¼Œå°†å†…å­˜æ›´ç´§å¯†åœ°é›†æˆåˆ°å¤„ç†å•å…ƒé™„è¿‘ï¼Œæˆ–ä¼˜åŒ–èŠ¯ç‰‡æ¶æ„ä»¥æ›´å¥½åœ°é€‚åº”LLMç®—æ³•çš„æ•°æ®æµï¼Œå¯ä»¥æ˜¾è‘—é™ä½å»¶è¿Ÿå’Œèƒ½è€—ã€‚è¿™ä¸€æ–¹æ³•åœ¨æœ€è¿‘çš„GPUå‘å±•ä¸­å·²æœ‰ä½“ç°ï¼Œå¦‚NVIDIAçš„Hopperæ¶æ„ï¼Œå®ƒåœ¨HBMå’ŒSRAMå®¹é‡ã€å†…å­˜å¸¦å®½ã€è®¡ç®—å•å…ƒå’Œåˆ†å‰²å¸¦å®½æ–¹é¢å–å¾—äº†æ”¹è¿›ï¼Œç›´æ¥æœ‰åˆ©äºLLMçš„å¤„ç†ã€‚

åœ¨è¿™ä¸€é¢†åŸŸçš„æŒç»­åˆ›æ–°å¯èƒ½åŒ…æ‹¬è®¾è®¡æœ¬è´¨ä¸Šé’ˆå¯¹ç”Ÿæˆå¼LLMè®¡ç®—æ¨¡å¼çš„ç¡¬ä»¶ï¼Œæ¯”å¦‚é’ˆå¯¹è¿™äº›æ¨¡å‹ä¸­å¸¸è§çš„æ³¨æ„åŠ›æœºåˆ¶å’Œå¼ é‡æ“ä½œçš„å…·ä½“éœ€æ±‚è¿›è¡Œä¼˜åŒ–ï¼Œæœ€ç»ˆå½±å“LLMæœåŠ¡ç³»ç»Ÿçš„è®¾è®¡å’Œå®æ–½ã€‚

NVIDIA H100 Tensor Core GPU Architecture. https://resources.nvidia.com/en-us-tensor-core/gtc22 whitepaper-hopper.

### é«˜æ•ˆä¸”æœ‰æ•ˆçš„è§£ç ç®—æ³•

å¼€å‘æ›´é«˜æ•ˆçš„è§£ç ç®—æ³•å¯ä»¥å¤§å¹…æå‡æœåŠ¡çš„æ•ˆç‡ã€‚é‰´äºå¯¹æ›´æœ‰æ•ˆåœ°åˆ©ç”¨LLMä¸­æ‰€åŒ…å«çš„ä¸°å¯ŒçŸ¥è¯†çš„è¿«åˆ‡éœ€æ±‚ï¼Œæœªæ¥çš„ç ”ç©¶å¯ä»¥æ¢ç´¢ä¸åŒäºä¼ ç»Ÿè‡ªå›å½’æ–¹æ³•çš„æ–°é€”å¾„ï¼Œä»¥å®ç°å®æ—¶åº”ç”¨ä¸­çš„ç”Ÿæˆé€Ÿåº¦æå‡ï¼ŒåŒæ—¶ä¿æŒè§£ç è´¨é‡ã€‚

ä¸€ä¸ªå……æ»¡å¸Œæœ›çš„ç ”ç©¶æ–¹å‘æ˜¯å¹¿ä¹‰æ¨æµ‹æ¨ç†ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿåœ¨ä¿æŒç”Ÿæˆè´¨é‡çš„åŒæ—¶æé«˜æ•ˆç‡ã€‚å…·ä½“è€Œè¨€ï¼Œå¯ä»¥å°†å°å‹æ¨æµ‹æ¨¡å‹æ³›åŒ–åˆ°ä»»ä½•èƒ½å¤Ÿæ¯”LLMæ›´é«˜æ•ˆåœ°ç”Ÿæˆåˆæ­¥ä»¤ç‰Œçš„å…¶ä»–æ–¹æ³•ï¼Œä¾‹å¦‚çŸ¥è¯†æ£€ç´¢å™¨å’Œç”¨æˆ·å®šä¹‰çš„å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œæœ€è¿‘çš„ä¸€äº›ç ”ç©¶å·¥ä½œå¼€å§‹ä½¿ç”¨æ—©æœŸé€€å‡ºæˆ–éè‡ªå›å½’è§£ç æ¥æ›¿ä»£åˆæ­¥æ¨¡å‹ã€‚

æ€»ç»“æ¥è¯´ï¼Œå¼€å‘åƒæ¨æµ‹è§£ç è¿™æ ·çš„é«˜æ•ˆè§£ç ç®—æ³•ï¼Œå¹¶ç»“åˆåº•å±‚ç³»ç»Ÿçš„ä¼˜åŒ–ï¼Œæ˜¯æå‡ç”Ÿæˆå¼LLMæœåŠ¡æ•ˆç‡çš„ä¸€ä¸ªé‡è¦æœºé‡ã€‚

SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification.

Inference with reference: Lossless acceleration of large language models.

Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding.

SPEED: Speculative Pipelined Execution for Efficient Decoding.

Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding.

Draft & Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding.

Breaking the Sequential Dependency of LLM Inference Using Lookahead Decoding.

Lossless acceleration for Seq2seq generation with aggressive decoding.

### é•¿ä¸Šä¸‹æ–‡/åºåˆ—åœºæ™¯çš„ä¼˜åŒ–ã€‚

éšç€LLMçš„åº”ç”¨å‘æ›´å¤æ‚çš„åœºæ™¯æ‰©å±•ï¼Œå¤„ç†æ›´é•¿ä¸Šä¸‹æ–‡æˆ–åºåˆ—çš„éœ€æ±‚æŒç»­ä¸Šå‡ã€‚åœ¨å¤„ç†é•¿åºåˆ—å·¥ä½œè´Ÿè½½çš„LLMæœåŠ¡ä¸­ï¼Œéœ€è¦ä»ç®—æ³•å’Œç³»ç»Ÿä¸¤ä¸ªæ–¹é¢è§£å†³æŒ‘æˆ˜ã€‚å¯¹äºLLMæ¥è¯´ï¼Œå½“åºåˆ—é•¿åº¦è¶…è¿‡è®­ç»ƒæœŸé—´æ‰€è§‚å¯Ÿåˆ°çš„é•¿åº¦æ—¶ï¼Œå®ƒä»¬å¸¸å¸¸ä¼šå‡ºç°é•¿åº¦æ³›åŒ–å¤±æ•ˆçš„é—®é¢˜ï¼Œå³ä½¿å¯ç”¨äº†ç›¸å¯¹ä½ç½®ç¼–ç æˆ–åœ¨æ›´é•¿çš„è¯­æ–™åº“ä¸Šè¿›è¡Œå¾®è°ƒä¹‹åã€‚å³ä¾¿æ˜¯æŸäº›å£°ç§°æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡çš„æ¨¡å‹ï¼Œç ”ç©¶ä¹Ÿå‘ç°å®ƒä»¬ä¼šé‡åˆ°â€œä¸­é—´æŸå¤±â€çš„é—®é¢˜ã€‚å½“å‰çš„æ–¹æ³•è¯•å›¾é€šè¿‡å‡å°‘è®¡ç®—åºåˆ—é•¿åº¦çš„åŒæ—¶ä¿ç•™ç›¸å…³ä¿¡æ¯æ¥å‡è½»è¿™äº›é™åˆ¶ï¼Œæ¯”å¦‚é‡‡ç”¨æ£€ç´¢å¢å¼ºã€åºåˆ—å‹ç¼©å’Œç¼“å­˜ç­‰æŠ€æœ¯ã€‚å¯¹äºLLMæœåŠ¡ç³»ç»Ÿæ¥è¯´ï¼Œé•¿åºåˆ—å¸¦æ¥äº†é‡è¦æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬å¢åŠ çš„å†…å­˜æ¶ˆè€—ã€KVç¼“å­˜çš„è®¿é—®ä»¥åŠè‡ªæ³¨æ„åŠ›è®¡ç®—å¤æ‚æ€§çš„æˆå€å¢åŠ ã€‚

Test Long: Attention with Linear Biases Enables Input Length Extrapolation.

Extending context window of large language models via positional interpolation.

LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding.

Lost in the middle: How language models use long contexts.

Retrieval meets Long Context Large Language Models.

LongLLM Lingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression.

Prompt Cache: Modular Attention Reuse for Low-Latency Inference.

### æ¢ç©¶æ›¿ä»£æ¶æ„

å°½ç®¡ç›®å‰Transformeræ¨¡å‹å’Œè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½ï¼Œä½†æ˜¯æ¢ç´¢æ›¿ä»£æ¶æ„æ˜¯æœªæ¥ç ”ç©¶çš„ä¸€ä¸ªå……æ»¡å¸Œæœ›çš„æ–¹å‘ã€‚åœ¨æ·±åº¦å­¦ä¹ ï¼ˆDLï¼‰é¢†åŸŸçš„å†å²ä¸Šï¼Œæˆ‘ä»¬è§è¯äº†ä¸»å¯¼æ¶æ„çš„æŒç»­æ›´è¿­ï¼Œæ¯ä¸€æ¬¡èŒƒå¼çš„è½¬å˜éƒ½ä¼šå¸¦æ¥é‡å¤§çš„è¿›æ­¥ã€‚åŸºäºè¿™ç§è¶‹åŠ¿ï¼Œè€ƒè™‘å…¶ä»–å¯èƒ½å¸¦æ¥ç‹¬ç‰¹ä¼˜åŠ¿çš„æ¶æ„æ–¹æ³•å°¤ä¸ºé‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨æé«˜è®¡ç®—æ•ˆç‡æ–¹é¢ã€‚ä¾‹å¦‚ï¼Œä¸€äº›æœ€æ–°çš„ç ”ç©¶æ­£åœ¨æ¢ç´¢æ— æ³¨æ„åŠ›æ–¹æ³•ï¼Œä½¿ç”¨çº¯å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰æ¶æ„æ¥æ›¿ä»£æ³¨æ„åŠ›æœºåˆ¶ã€‚æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹æ¶æ„çš„æ¼”å˜ä¸ä»…æ˜¯ä¸€ç§è‡ªç„¶çš„å‘å±•è¿‡ç¨‹ï¼Œä¹Ÿæ˜¯ä¸ºäº†å‘ç°æ›´é«˜æ•ˆå’Œæœ‰æ•ˆçš„LLMç»“æ„æ–¹å¼çš„å¿…è¦æ¢ç´¢ã€‚

Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers.

### æ¢ç´¢åœ¨å¤æ‚ç¯å¢ƒä¸­çš„éƒ¨ç½²

éšç€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº”ç”¨çš„ä¸æ–­æ‰©å±•ï¼Œä¸€ä¸ªè‡³å…³é‡è¦çš„æœªæ¥æ–¹å‘æ˜¯æ¢ç´¢å’Œä¼˜åŒ–å®ƒä»¬åœ¨ä¸åŒå¤æ‚ç¯å¢ƒä¸­çš„éƒ¨ç½²ã€‚è¿™ç§æ¢ç´¢ä¸ä»…é™äºä¼ ç»Ÿçš„åŸºäºäº‘çš„éƒ¨ç½²ï¼Œè¿˜åŒ…æ‹¬è¾¹ç¼˜è®¡ç®—ã€æ··åˆè®¡ç®—ï¼ˆç»“åˆäº‘å’Œè¾¹ç¼˜è®¡ç®—ï¼‰ã€å»ä¸­å¿ƒåŒ–è®¡ç®—ï¼Œä»¥åŠåˆ©ç”¨æ›´ç»æµçš„èµ„æºï¼Œå¦‚æŠ¢å å¼å®ä¾‹ã€‚æ¯ä¸€ç§ç¯å¢ƒéƒ½ä¸ºLLMæœåŠ¡å¸¦æ¥äº†ç‹¬ç‰¹çš„æŒ‘æˆ˜å’Œæœºé‡ã€‚ä¾‹å¦‚ï¼Œè¾¹ç¼˜è®¡ç®—é€šè¿‡åœ¨æ•°æ®æºé™„è¿‘å¤„ç†æ•°æ®ï¼Œå¯ä»¥å®ç°æ›´å¿«çš„å“åº”æ—¶é—´å’Œå‡å°‘å¸¦å®½æ¶ˆè€—ï¼Œä½†åŒæ—¶å®ƒä¹Ÿé¢ä¸´ç€è®¡ç®—èµ„æºå’Œå­˜å‚¨å®¹é‡æœ‰é™çš„é—®é¢˜ã€‚æ··åˆè®¡ç®—æä¾›äº†ä¸€ç§å¹³è¡¡çš„æ–¹æ³•ï¼Œä½†éœ€è¦å…ˆè¿›çš„ç®¡ç†ç­–ç•¥æ¥æœ‰æ•ˆåœ°åˆ†é…è®¡ç®—ä»»åŠ¡ã€‚å»ä¸­å¿ƒåŒ–è®¡ç®—ä¸ºä¼—åŒ…è®¡ç®—èµ„æºæä¾›äº†ä¸€æ¡æœ‰å¸Œæœ›çš„é“è·¯ï¼Œä½†ä¹Ÿå¸¦æ¥äº†æ•°æ®éšç§å’Œå®‰å…¨çš„é¢å¤–è€ƒé‡ã€‚åœ¨æŠ¢å å¼èµ„æºä¸Šæä¾›LLMæœåŠ¡å¯ä»¥æ˜¾è‘—é™ä½æˆæœ¬ï¼Œä½†éœ€è¦å®¹é”™æœºåˆ¶æ¥åº”å¯¹å…¶å›ºæœ‰çš„ä¸å¯é¢„æµ‹æ€§å’Œå˜å¼‚æ€§ï¼Œä»¥ç¡®ä¿æ€§èƒ½çš„ç¨³å®šå’Œç³»ç»Ÿçš„å¯é æ€§ã€‚æˆåŠŸåº”å¯¹è¿™äº›å¤æ‚ç¯å¢ƒä¸­çš„æŒ‘æˆ˜ï¼Œå°†æ˜¯å®ç°æ›´åŠ å¼ºå¤§ã€å¯æ‰©å±•ä¸”é«˜æ•ˆçš„LLMåº”ç”¨çš„å…³é”®ã€‚

The future of AI is hybrid. https://www.qualcomm.com/content/dam/qcomm-martech/dm assets/documents/Whitepaper-The-future-of-AI-is-hybrid-Part-2-Qualcomm-is-uniquely-positioned-to-scale hybrid-AI.pdf.

BumbleBee: Secure Two-party Inference Framework for Large Transformers.

LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud.

SpotServe: Serving Generative Large Language Models on Preemptible Instances.

### è‡ªåŠ¨é€‚åº”ç‰¹å®šéœ€æ±‚

å¤šæ ·åŒ–çš„åº”ç”¨ç‰¹å®šéœ€æ±‚åˆ›é€ äº†å¹¿æ³›çš„åˆ›æ–°LLMæœåŠ¡ä¼˜åŒ–æœºé‡ï¼Œä¾‹å¦‚å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œä»å¤–éƒ¨å‘é‡å­˜å‚¨ä¸­è¿›è¡Œæ£€ç´¢ï¼Œåœ¨çº¿å­¦ä¹ å’ŒçŸ¥è¯†æ›´æ–°ï¼Œå¤šæ¨¡æ€å·¥ä½œè´Ÿè½½ï¼Œä»¥åŠå°†ä¸åŒLLMçš„èƒ½åŠ›ä¸²è”èµ·æ¥ã€‚è¿™äº›ç‹¬ç‰¹çš„æŒ‘æˆ˜ä¹Ÿè¦æ±‚èƒ½å¤Ÿè‡ªåŠ¨ä¸”æ— ç¼åœ°å°†LLMæœåŠ¡æŠ€æœ¯é›†æˆåˆ°ç°æœ‰çš„ITåŸºç¡€è®¾æ–½ä¸­ï¼Œé€šè¿‡å°†ä¼˜åŒ–èŒƒå›´æ‰©å±•åˆ°æ•´ä¸ªLLMç”Ÿå‘½å‘¨æœŸï¼ŒåŒ…æ‹¬æ•°æ®é‡‡é›†å’Œå¤„ç†ï¼Œè‡ªåŠ¨æœºå™¨å­¦ä¹ ï¼ˆAutoMLï¼‰å’Œæ¨¡å‹ç®¡ç†ï¼Œèµ„æºåˆ†é…ï¼Œä»¥åŠæ€§èƒ½ç›‘æ§ã€‚

Punica: Multi-Tenant LoRA Serving.

S-LoRA: Serving Thousands of Concurrent LoRA Adapters.

PetS: A Unified Framework for Parameter Efficient Transformers Serving.

Improving language models by retrieving from trillions of tokens.

Autogen: Enabling next-gen llm applications via multi-agent conversation framework.

AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks.

Saturn: An Optimized Data System for Large Model Deep Learning Workloads.

