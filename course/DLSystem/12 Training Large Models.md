---
date: 2024-11-01 19:06:45
date modified: 2024-11-01 19:46:53
title: Welcome to use Hexo Theme Keep
tags:
  - Hexo
  - Keep
categories:
  - Hexo
---
## Techniques for memory saving

Can we fit as big of neural network training onto the limited memory devices?

### Source of memory consumption

Sources of memory consumption

- Model weights

- Optimizer states

- Intermediate activation values

![image](https://github.com/amor-mio-de-mi-vida/picx-images-hosting/raw/master/dlsystem/image.51e5q74d6i.webp)

## Parallel and distributed training

### Advanced parallelization methods

ZeRO: Memory Optimizations Toward Training Trillion Parameter Models.

Beyond Data and Model Parallelism for Deep Neural Networks.

GSPMD: General and Scalable Parallelization for ML Computation Graphs.